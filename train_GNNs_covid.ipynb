{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oklaja/Virtual-Drug-Screening-for-Covid-19/blob/main/train_GNNs_covid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BzIaLbBcSf2",
        "outputId": "e0f2d19e-2240-440d-e4c2-6d96b3eba8fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.3.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.7 MB 82.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (7.1.2)\n",
            "Installing collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.3.3\n"
          ]
        }
      ],
      "source": [
        "# Install rdkit\n",
        "\n",
        "# !curl -L bit.ly/rdkit-colab | tar xz -C /\n",
        "!pip install rdkit-pypi"
      ],
      "id": "6BzIaLbBcSf2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fKDJNQbr6A9",
        "outputId": "933cb001-088d-4aa9-d9bd-6a41bb60d1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 34.9 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse, torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9 torch-sparse-0.6.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 36.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_geometric) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch_geometric) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=68eba6e99344264172fec6d40042f4f0603c389da787f8dd782f41f78781523a\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n"
          ]
        }
      ],
      "source": [
        "# Install compatible versions of torch geometric and its dependencies\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch_geometric"
      ],
      "id": "6fKDJNQbr6A9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1ha01xTiaKH",
        "outputId": "ed3ea27f-8f3d-4c7b-fdca-e7c735b31b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount your google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "P1ha01xTiaKH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OovVx5G5jCkq",
        "outputId": "9c8bbfa2-ce52-4874-ea2c-57e90ccb2d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/AI/datasets/covid/datasetF/test/"
      ],
      "id": "OovVx5G5jCkq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Attentive FP"
      ],
      "metadata": {
        "id": "GaONiPXKjY2E"
      },
      "id": "GaONiPXKjY2E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "011ee9c8-eded-4e7f-baa2-ba0399f51fe6",
        "outputId": "20e287a2-d93d-454d-b760-b20160707203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AttentiveFP(\n",
            "  (lin1): Linear(in_features=44, out_features=250, bias=True)\n",
            "  (atom_convs): ModuleList(\n",
            "    (0): GATEConv()\n",
            "    (1): GATConv(250, 250, heads=1)\n",
            "    (2): GATConv(250, 250, heads=1)\n",
            "    (3): GATConv(250, 250, heads=1)\n",
            "  )\n",
            "  (atom_grus): ModuleList(\n",
            "    (0): GRUCell(250, 250)\n",
            "    (1): GRUCell(250, 250)\n",
            "    (2): GRUCell(250, 250)\n",
            "    (3): GRUCell(250, 250)\n",
            "  )\n",
            "  (mol_conv): GATConv(250, 250, heads=1)\n",
            "  (mol_gru): GRUCell(250, 250)\n",
            "  (lin2): Linear(in_features=250, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Import dependencies\n",
        "\n",
        "import os.path as osp\n",
        "from math import sqrt\n",
        " \n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from rdkit import Chem\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import MoleculeNet\n",
        "from torch_geometric.nn.models import AttentiveFP\n",
        "import torch\n",
        "from sklearn import metrics\n",
        "\n",
        "# Build a Torch Geometric datasets from CSV with SMILES and binary Activity columns\n",
        "# Generate features and save Graph datasets for training, validation and testing\n",
        " \n",
        "class GenFeatures(object):\n",
        "    def __init__(self):\n",
        "        self.symbols = [\n",
        "            'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br',\n",
        "            'Te', 'I', 'At', 'H', 'Na', 'K', 'Al', 'other'\n",
        "        ]\n",
        " \n",
        "        self.hybridizations = [\n",
        "            Chem.rdchem.HybridizationType.S,\n",
        "            Chem.rdchem.HybridizationType.SP,\n",
        "            Chem.rdchem.HybridizationType.SP2,\n",
        "            Chem.rdchem.HybridizationType.SP3,\n",
        "            Chem.rdchem.HybridizationType.SP3D,\n",
        "            Chem.rdchem.HybridizationType.SP3D2,\n",
        "            'other',\n",
        "        ]\n",
        " \n",
        "        self.stereos = [\n",
        "            Chem.rdchem.BondStereo.STEREONONE,\n",
        "            Chem.rdchem.BondStereo.STEREOANY,\n",
        "            Chem.rdchem.BondStereo.STEREOZ,\n",
        "            Chem.rdchem.BondStereo.STEREOE,\n",
        "        ]\n",
        " \n",
        "    def __call__(self, data):\n",
        "        # Generate AttentiveFP features according to Table 1.\n",
        "        mol = Chem.MolFromSmiles(data.smiles)\n",
        " \n",
        "        xs = []\n",
        "        for atom in mol.GetAtoms():\n",
        "            symbol = [0.] * len(self.symbols)\n",
        "            symbol[self.symbols.index(atom.GetSymbol())] = 1.\n",
        "            degree = [0.] * 6\n",
        "            degree[atom.GetDegree()] = 1.\n",
        "            formal_charge = atom.GetFormalCharge()\n",
        "            radical_electrons = atom.GetNumRadicalElectrons()\n",
        "            hybridization = [0.] * len(self.hybridizations)\n",
        "            hybridization[self.hybridizations.index(\n",
        "                atom.GetHybridization())] = 1.\n",
        "            aromaticity = 1. if atom.GetIsAromatic() else 0.\n",
        "            hydrogens = [0.] * 5\n",
        "            hydrogens[atom.GetTotalNumHs()] = 1.\n",
        "            chirality = 1. if atom.HasProp('_ChiralityPossible') else 0.\n",
        "            chirality_type = [0.] * 2\n",
        "            if atom.HasProp('_CIPCode'):\n",
        "                chirality_type[['R', 'S'].index(atom.GetProp('_CIPCode'))] = 1.\n",
        " \n",
        "            x = torch.tensor(symbol + degree + [formal_charge] +\n",
        "                             [radical_electrons] + hybridization +\n",
        "                             [aromaticity] + hydrogens + [chirality] +\n",
        "                             chirality_type)\n",
        "            xs.append(x)\n",
        " \n",
        "        data.x = torch.stack(xs, dim=0)\n",
        " \n",
        "        edge_indices = []\n",
        "        edge_attrs = []\n",
        "        for bond in mol.GetBonds():\n",
        "            edge_indices += [[bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()]]\n",
        "            edge_indices += [[bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()]]\n",
        " \n",
        "            bond_type = bond.GetBondType()\n",
        "            single = 1. if bond_type == Chem.rdchem.BondType.SINGLE else 0.\n",
        "            double = 1. if bond_type == Chem.rdchem.BondType.DOUBLE else 0.\n",
        "            triple = 1. if bond_type == Chem.rdchem.BondType.TRIPLE else 0.\n",
        "            aromatic = 1. if bond_type == Chem.rdchem.BondType.AROMATIC else 0.\n",
        "            conjugation = 1. if bond.GetIsConjugated() else 0.\n",
        "            ring = 1. if bond.IsInRing() else 0.\n",
        "            stereo = [0.] * 4\n",
        "            stereo[self.stereos.index(bond.GetStereo())] = 1.\n",
        " \n",
        "            edge_attr = torch.tensor(\n",
        "                [single, double, triple, aromatic, conjugation, ring] + stereo)\n",
        " \n",
        "            edge_attrs += [edge_attr, edge_attr]\n",
        " \n",
        "        if len(edge_attrs) == 0:\n",
        "            data.edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
        "            data.edge_attr = torch.zeros((0, 10), dtype=torch.float)\n",
        "        else:\n",
        "            data.edge_index = torch.tensor(edge_indices).t().contiguous()\n",
        "            data.edge_attr = torch.stack(edge_attrs, dim=0)\n",
        " \n",
        "        return data\n",
        " \n",
        " \n",
        "x_map = {\n",
        "    'atomic_num':\n",
        "    list(range(0, 119)),\n",
        "    'chirality': [\n",
        "        'CHI_UNSPECIFIED',\n",
        "        'CHI_TETRAHEDRAL_CW',\n",
        "        'CHI_TETRAHEDRAL_CCW',\n",
        "        'CHI_OTHER',\n",
        "    ],\n",
        "    'degree':\n",
        "    list(range(0, 11)),\n",
        "    'formal_charge':\n",
        "    list(range(-5, 7)),\n",
        "    'num_hs':\n",
        "    list(range(0, 9)),\n",
        "    'num_radical_electrons':\n",
        "    list(range(0, 5)),\n",
        "    'hybridization': [\n",
        "        'UNSPECIFIED',\n",
        "        'S',\n",
        "        'SP',\n",
        "        'SP2',\n",
        "        'SP3',\n",
        "        'SP3D',\n",
        "        'SP3D2',\n",
        "        'OTHER',\n",
        "    ],\n",
        "    'is_aromatic': [False, True],\n",
        "    'is_in_ring': [False, True],\n",
        "}\n",
        " \n",
        "e_map = {\n",
        "    'bond_type': [\n",
        "        'misc',\n",
        "        'SINGLE',\n",
        "        'DOUBLE',\n",
        "        'TRIPLE',\n",
        "        'AROMATIC',\n",
        "    ],\n",
        "    'stereo': [\n",
        "        'STEREONONE',\n",
        "        'STEREOZ',\n",
        "        'STEREOE',\n",
        "        'STEREOCIS',\n",
        "        'STEREOTRANS',\n",
        "        'STEREOANY',\n",
        "    ],\n",
        "    'is_conjugated': [False, True],\n",
        "}\n",
        " \n",
        " \n",
        "import torch\n",
        "from torch_geometric.data import (InMemoryDataset, Data)\n",
        "import re\n",
        " \n",
        "class Molecule(InMemoryDataset):\n",
        "    r\"\"\"Customized processing of MoleculeNet Dataset:\n",
        "\n",
        "    The `MoleculeNet <http://moleculenet.ai/datasets-1>`_ benchmark\n",
        "    collection  from the `\"MoleculeNet: A Benchmark for Molecular Machine\n",
        "    Learning\" <https://arxiv.org/abs/1703.00564>`_ paper, containing datasets\n",
        "    from physical chemistry, biophysics and physiology.\n",
        "    All datasets come with the additional node and edge features introduced by\n",
        "    the `Open Graph Benchmark <https://ogb.stanford.edu/docs/graphprop/>`_.\n",
        "    Args:\n",
        "        root_dir (string): Root directory.\n",
        "        name (string): The name of dataset (csv format)\n",
        "        smi_idx (integer): index of smiles column\n",
        "        target_idx (integer): index of target column\n",
        "        transform (callable, optional): A function/transform that takes in an\n",
        "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
        "            version. The data object will be transformed before every access.\n",
        "            (default: :obj:`None`)\n",
        "        pre_transform (callable, optional): A function/transform that takes in\n",
        "            an :obj:`torch_geometric.data.Data` object and returns a\n",
        "            transformed version. The data object will be transformed before\n",
        "            being saved to disk. (default: :obj:`None`)\n",
        "        pre_filter (callable, optional): A function that takes in an\n",
        "            :obj:`torch_geometric.data.Data` object and returns a boolean\n",
        "            value, indicating whether the data object should be included in the\n",
        "            final dataset. (default: :obj:`None`)\n",
        "    \"\"\"\n",
        " \n",
        " \n",
        " \n",
        "    def __init__(self, root_dir, name, smi_idx, target_idx, transform=None, \n",
        "                 pre_transform=None, pre_filter=None):\n",
        "      \n",
        "        self.root_dir = root_dir\n",
        "        self.name = name\n",
        "        self.smi_idx = smi_idx\n",
        "        self.target_idx = target_idx\n",
        "        #skip calling data\n",
        "        super(Molecule, self).__init__(None, transform, pre_transform, pre_filter)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        " \n",
        "    @property\n",
        "    def raw_dir(self):\n",
        "        return osp.join(self.root_dir, 'raw')\n",
        " \n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "        return osp.join(self.root_dir,'processed')\n",
        " \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return f'{self.name}'\n",
        " \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'data.pt'\n",
        " \n",
        " \n",
        "    def process(self):\n",
        "        from rdkit import Chem\n",
        "        with open(self.raw_file_names, 'r') as f:\n",
        "            dataset = f.read().split('\\n')[1:-1]\n",
        "            dataset = [x for x in dataset if len(x) > 0]  # Filter empty lines.\n",
        " \n",
        "        data_list = []\n",
        "        for line in dataset:\n",
        "            # if not line.startswith(\"smiles\"): # in line:\n",
        "            try:\n",
        "            \n",
        "                line = re.sub(r'\\\".*\\\"', '', line)  # Replace \".*\" strings.\n",
        "                line = line.split(',')\n",
        "\n",
        "                smiles = line[self.smi_idx]\n",
        "                ys = line[self.target_idx]\n",
        "                ys = float(ys)\n",
        "                y = torch.tensor(ys, dtype=torch.float).view(1, -1)\n",
        "\n",
        "                mol = Chem.MolFromSmiles(smiles)\n",
        "                if mol is None:\n",
        "                    continue\n",
        "\n",
        "                xs = []\n",
        "                for atom in mol.GetAtoms():\n",
        "                    x = []\n",
        "                    x.append(x_map['atomic_num'].index(atom.GetAtomicNum()))\n",
        "                    x.append(x_map['chirality'].index(str(atom.GetChiralTag())))\n",
        "                    x.append(x_map['degree'].index(atom.GetTotalDegree()))\n",
        "                    x.append(x_map['formal_charge'].index(atom.GetFormalCharge()))\n",
        "                    x.append(x_map['num_hs'].index(atom.GetTotalNumHs()))\n",
        "                    x.append(x_map['num_radical_electrons'].index(\n",
        "                        atom.GetNumRadicalElectrons()))\n",
        "                    x.append(x_map['hybridization'].index(\n",
        "                        str(atom.GetHybridization())))\n",
        "                    x.append(x_map['is_aromatic'].index(atom.GetIsAromatic()))\n",
        "                    x.append(x_map['is_in_ring'].index(atom.IsInRing()))\n",
        "                    xs.append(x)\n",
        "\n",
        "                x = torch.tensor(xs, dtype=torch.long).view(-1, 9)\n",
        "\n",
        "                edge_indices, edge_attrs = [], []\n",
        "                for bond in mol.GetBonds():\n",
        "                    i = bond.GetBeginAtomIdx()\n",
        "                    j = bond.GetEndAtomIdx()\n",
        "\n",
        "                    e = []\n",
        "                    e.append(e_map['bond_type'].index(str(bond.GetBondType())))\n",
        "                    e.append(e_map['stereo'].index(str(bond.GetStereo())))\n",
        "                    e.append(e_map['is_conjugated'].index(bond.GetIsConjugated()))\n",
        "\n",
        "                    edge_indices += [[i, j], [j, i]]\n",
        "                    edge_attrs += [e, e]\n",
        "\n",
        "                edge_index = torch.tensor(edge_indices)\n",
        "                edge_index = edge_index.t().to(torch.long).view(2, -1)\n",
        "                edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 3)\n",
        "\n",
        "                # Sort indices.\n",
        "                if edge_index.numel() > 0:\n",
        "                    perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()\n",
        "                    edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]\n",
        "\n",
        "                data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y,\n",
        "                            smiles=smiles)\n",
        "\n",
        "                if self.pre_filter is not None and not self.pre_filter(data):\n",
        "                    continue\n",
        "\n",
        "                if self.pre_transform is not None:\n",
        "                    data = self.pre_transform(data)\n",
        "\n",
        "                data_list.append(data)\n",
        "                \n",
        "                \n",
        "            except: pass\n",
        "\n",
        "            torch.save(self.collate(data_list), self.processed_paths[0])\n",
        "\n",
        "        def __repr__(self):\n",
        "            return '{}({})'.format(self.names[self.name][0], len(self))\n",
        "\n",
        "\n",
        "\n",
        "# Build or load the datasets \n",
        "\n",
        "\n",
        "train_dataset = Molecule(root_dir='/content/drive/MyDrive/AI/datasets/covid/datasetF/',\n",
        "                  name='./trainC.csv',\n",
        "                  smi_idx=1,\n",
        "                  target_idx=2,\n",
        "                  pre_transform=GenFeatures()).shuffle()\n",
        "\n",
        "\n",
        "test_dataset = Molecule(root_dir='/content/drive/MyDrive/AI/datasets/covid/datasetF/test/',\n",
        "                  name='./testC.csv',\n",
        "                  smi_idx=1,\n",
        "                  target_idx=2,\n",
        "                  pre_transform=GenFeatures()).shuffle()\n",
        "\n",
        "val_dataset = Molecule(root_dir='/content/drive/MyDrive/AI/datasets/covid/datasetF/val/',\n",
        "                  name='./valC.csv',\n",
        "                  smi_idx=1,\n",
        "                  target_idx=2,\n",
        "                  pre_transform=GenFeatures()).shuffle()\n",
        "\n",
        "\n",
        "# Initiate DataLoader object\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4096, \n",
        "                          shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4096, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4096, num_workers=0)\n",
        "\n",
        "\n",
        "# Set optimizer parameters\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=10**-3.5,\n",
        "                             weight_decay=10**-2.9)\n",
        "\n",
        "\n",
        "# Set weight for loss scaling, set printing of results to preferred number of\n",
        "# samples or 0 (every epoch):\n",
        "\n",
        "WEIGHT = 300.0\n",
        "print_every = 0  # 0 for every epoch\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    \n",
        "    total_pred = [] \n",
        "    total_y = []\n",
        "    total_loss = total_examples = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        out = out.sigmoid()\n",
        "\n",
        "        weight = torch.tensor([WEIGHT], dtype=torch.float)\n",
        "        loss = F.binary_cross_entropy(out, data.y, weight=weight.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        total_examples += data.num_graphs\n",
        "        total_pred.append(out.detach().cpu().numpy())\n",
        "        total_y.append(data.y.detach().cpu().numpy())\n",
        "        \n",
        "        if print_every != 0 and i % print_every == 0:\n",
        "            val_loss, val_roc = test(val_loader)\n",
        "            test_loss, test_roc = test(test_loader)\n",
        "            fpr, tpr, threshold = metrics.roc_curve(np.concatenate(total_y, 0), \n",
        "                                                    np.concatenate(total_pred, 0))\n",
        "            roc_auc = metrics.auc(fpr, tpr)\n",
        "            print(f'Epoch: {epoch:03d} | LOSS: Train: {total_loss / total_examples:.4f} Val: {val_loss:.4f} Test: {test_loss:.4f}  | ROC: Train: {roc_auc:.3f} Val: {val_roc:.3f}, '\n",
        "                  f'Test: {test_roc:.3f}')\n",
        "            \n",
        "        \n",
        "    fpr, tpr, threshold = metrics.roc_curve(np.concatenate(total_y, 0), \n",
        "                                            np.concatenate(total_pred, 0))\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return total_loss / total_examples, roc_auc\n",
        " \n",
        "\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "    total_pred = []\n",
        "    total_y = []\n",
        "    \n",
        "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)  \n",
        "        pred = torch.sigmoid(out)  #  out.sigmoid()\n",
        "        total_pred.append(pred.detach().cpu().numpy())\n",
        "        total_y.append(data.y.detach().cpu().numpy())\n",
        "\n",
        "            \n",
        "        weight = torch.tensor([WEIGHT], dtype=torch.float)\n",
        "        loss_test = F.binary_cross_entropy(pred, data.y, weight=weight.to(device))\n",
        "        total_loss += float(loss_test)  * data.num_graphs\n",
        "        total_examples += data.num_graphs\n",
        "\n",
        "    fpr, tpr, threshold = metrics.roc_curve(np.concatenate(total_y, 0), \n",
        "                                            np.concatenate(total_pred, 0))\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return total_loss / total_examples , roc_auc\n",
        "    "
      ],
      "id": "011ee9c8-eded-4e7f-baa2-ba0399f51fe6"
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset), len(val_dataset), len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPAOMvOMP0tg",
        "outputId": "13e3b607-d37c-48a5-ff0f-7b47227a330b"
      },
      "id": "RPAOMvOMP0tg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234617 33517 67036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NWhQZfCpNfy",
        "outputId": "db487f7e-616a-4047-da0e-40050aeb3260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001 | LOSS: Train: 24.2889 Val: 7.6985 Test: 7.6301  | ROC: Train: 0.440, Val: 0.271, Test: 0.364\n",
            "Epoch: 002 | LOSS: Train: 7.3749 Val: 6.6244 Test: 6.8560  | ROC: Train: 0.536, Val: 0.710, Test: 0.631\n",
            "Epoch: 003 | LOSS: Train: 7.0393 Val: 6.5624 Test: 6.8216  | ROC: Train: 0.614, Val: 0.720, Test: 0.634\n",
            "Epoch: 004 | LOSS: Train: 6.9041 Val: 6.5502 Test: 6.8289  | ROC: Train: 0.631, Val: 0.717, Test: 0.644\n",
            "Epoch: 005 | LOSS: Train: 6.7473 Val: 6.4012 Test: 6.7633  | ROC: Train: 0.669, Val: 0.732, Test: 0.654\n",
            "Epoch: 006 | LOSS: Train: 6.7424 Val: 6.3948 Test: 6.7736  | ROC: Train: 0.664, Val: 0.737, Test: 0.669\n",
            "Epoch: 007 | LOSS: Train: 6.6201 Val: 6.3267 Test: 6.7040  | ROC: Train: 0.696, Val: 0.718, Test: 0.682\n",
            "Epoch: 008 | LOSS: Train: 6.4670 Val: 6.2125 Test: 6.5775  | ROC: Train: 0.710, Val: 0.730, Test: 0.693\n",
            "Epoch: 009 | LOSS: Train: 6.4344 Val: 6.1438 Test: 6.5513  | ROC: Train: 0.710, Val: 0.731, Test: 0.702\n",
            "Epoch: 010 | LOSS: Train: 6.4025 Val: 6.2087 Test: 6.5415  | ROC: Train: 0.709, Val: 0.731, Test: 0.715\n",
            "Epoch: 011 | LOSS: Train: 6.2756 Val: 6.0476 Test: 6.4911  | ROC: Train: 0.728, Val: 0.727, Test: 0.709\n",
            "Epoch: 012 | LOSS: Train: 6.2580 Val: 6.2041 Test: 6.5645  | ROC: Train: 0.737, Val: 0.738, Test: 0.725\n",
            "Epoch: 013 | LOSS: Train: 6.2942 Val: 6.1717 Test: 6.5755  | ROC: Train: 0.732, Val: 0.750, Test: 0.725\n",
            "Epoch: 014 | LOSS: Train: 6.2570 Val: 6.0311 Test: 6.4272  | ROC: Train: 0.739, Val: 0.740, Test: 0.734\n",
            "Epoch: 015 | LOSS: Train: 6.2029 Val: 5.9848 Test: 6.3936  | ROC: Train: 0.747, Val: 0.742, Test: 0.735\n",
            "Epoch: 016 | LOSS: Train: 6.1796 Val: 6.0072 Test: 6.4223  | ROC: Train: 0.750, Val: 0.742, Test: 0.738\n",
            "Epoch: 017 | LOSS: Train: 6.1607 Val: 6.1359 Test: 6.5427  | ROC: Train: 0.753, Val: 0.736, Test: 0.739\n",
            "Epoch: 018 | LOSS: Train: 6.1813 Val: 5.9759 Test: 6.3826  | ROC: Train: 0.745, Val: 0.753, Test: 0.749\n",
            "Epoch: 019 | LOSS: Train: 6.0970 Val: 5.9220 Test: 6.3576  | ROC: Train: 0.763, Val: 0.767, Test: 0.750\n",
            "Epoch: 020 | LOSS: Train: 6.0911 Val: 5.9367 Test: 6.3744  | ROC: Train: 0.765, Val: 0.760, Test: 0.757\n",
            "Epoch: 021 | LOSS: Train: 6.0430 Val: 5.9136 Test: 6.3033  | ROC: Train: 0.770, Val: 0.769, Test: 0.763\n",
            "Epoch: 022 | LOSS: Train: 6.0477 Val: 5.8709 Test: 6.2746  | ROC: Train: 0.771, Val: 0.770, Test: 0.765\n",
            "Epoch: 023 | LOSS: Train: 6.0185 Val: 5.8098 Test: 6.2330  | ROC: Train: 0.774, Val: 0.787, Test: 0.774\n",
            "Epoch: 024 | LOSS: Train: 5.9719 Val: 5.7951 Test: 6.2345  | ROC: Train: 0.786, Val: 0.797, Test: 0.777\n",
            "Epoch: 025 | LOSS: Train: 5.9506 Val: 6.0386 Test: 6.4618  | ROC: Train: 0.789, Val: 0.775, Test: 0.772\n",
            "Epoch: 026 | LOSS: Train: 5.9699 Val: 5.8596 Test: 6.2867  | ROC: Train: 0.786, Val: 0.771, Test: 0.772\n",
            "Epoch: 027 | LOSS: Train: 5.8869 Val: 5.7910 Test: 6.2178  | ROC: Train: 0.798, Val: 0.806, Test: 0.785\n",
            "Epoch: 028 | LOSS: Train: 5.8999 Val: 5.7677 Test: 6.2012  | ROC: Train: 0.792, Val: 0.784, Test: 0.776\n",
            "Epoch: 029 | LOSS: Train: 5.8736 Val: 5.7018 Test: 6.1926  | ROC: Train: 0.795, Val: 0.801, Test: 0.785\n",
            "Epoch: 030 | LOSS: Train: 5.8700 Val: 5.7184 Test: 6.1842  | ROC: Train: 0.801, Val: 0.802, Test: 0.782\n",
            "Epoch: 031 | LOSS: Train: 5.7947 Val: 5.6278 Test: 6.1200  | ROC: Train: 0.807, Val: 0.814, Test: 0.787\n",
            "Epoch: 032 | LOSS: Train: 5.7935 Val: 5.7154 Test: 6.1831  | ROC: Train: 0.807, Val: 0.819, Test: 0.792\n",
            "Epoch: 033 | LOSS: Train: 5.8211 Val: 5.5843 Test: 6.0872  | ROC: Train: 0.808, Val: 0.818, Test: 0.790\n",
            "Epoch: 034 | LOSS: Train: 5.8198 Val: 5.6069 Test: 6.1171  | ROC: Train: 0.799, Val: 0.820, Test: 0.790\n",
            "Epoch: 035 | LOSS: Train: 5.7650 Val: 5.5684 Test: 6.0930  | ROC: Train: 0.816, Val: 0.829, Test: 0.792\n",
            "Epoch: 036 | LOSS: Train: 5.7025 Val: 5.7262 Test: 6.2901  | ROC: Train: 0.816, Val: 0.812, Test: 0.782\n",
            "Epoch: 037 | LOSS: Train: 5.7028 Val: 5.5873 Test: 6.1120  | ROC: Train: 0.817, Val: 0.824, Test: 0.791\n",
            "Epoch: 038 | LOSS: Train: 5.6814 Val: 5.5727 Test: 6.0612  | ROC: Train: 0.812, Val: 0.836, Test: 0.796\n",
            "Epoch: 039 | LOSS: Train: 5.6187 Val: 5.5080 Test: 6.0085  | ROC: Train: 0.826, Val: 0.838, Test: 0.790\n",
            "Epoch: 040 | LOSS: Train: 5.6920 Val: 5.5251 Test: 5.9824  | ROC: Train: 0.811, Val: 0.836, Test: 0.789\n",
            "Epoch: 041 | LOSS: Train: 5.6243 Val: 5.7607 Test: 6.2007  | ROC: Train: 0.820, Val: 0.832, Test: 0.794\n",
            "Epoch: 042 | LOSS: Train: 5.5786 Val: 5.4794 Test: 6.0113  | ROC: Train: 0.821, Val: 0.844, Test: 0.798\n",
            "Epoch: 043 | LOSS: Train: 5.6951 Val: 5.6392 Test: 6.1206  | ROC: Train: 0.813, Val: 0.847, Test: 0.801\n",
            "Epoch: 044 | LOSS: Train: 5.5525 Val: 5.4896 Test: 6.0533  | ROC: Train: 0.826, Val: 0.845, Test: 0.804\n",
            "Epoch: 045 | LOSS: Train: 5.5225 Val: 5.4235 Test: 5.9995  | ROC: Train: 0.824, Val: 0.846, Test: 0.804\n",
            "Epoch: 046 | LOSS: Train: 5.4691 Val: 5.3944 Test: 5.9235  | ROC: Train: 0.834, Val: 0.853, Test: 0.804\n",
            "Epoch: 047 | LOSS: Train: 5.4351 Val: 5.3635 Test: 5.8796  | ROC: Train: 0.839, Val: 0.857, Test: 0.808\n",
            "Epoch: 048 | LOSS: Train: 5.4724 Val: 5.3819 Test: 5.8307  | ROC: Train: 0.827, Val: 0.854, Test: 0.808\n",
            "Epoch: 049 | LOSS: Train: 5.5353 Val: 5.3945 Test: 5.8963  | ROC: Train: 0.825, Val: 0.846, Test: 0.802\n",
            "Epoch: 050 | LOSS: Train: 5.4077 Val: 5.3538 Test: 5.7944  | ROC: Train: 0.835, Val: 0.846, Test: 0.800\n",
            "Epoch: 051 | LOSS: Train: 5.4224 Val: 5.5041 Test: 6.0294  | ROC: Train: 0.833, Val: 0.849, Test: 0.802\n",
            "Epoch: 052 | LOSS: Train: 5.4337 Val: 5.2294 Test: 5.7407  | ROC: Train: 0.831, Val: 0.860, Test: 0.815\n",
            "Epoch: 053 | LOSS: Train: 5.3750 Val: 5.2902 Test: 5.7770  | ROC: Train: 0.842, Val: 0.861, Test: 0.817\n",
            "Epoch: 054 | LOSS: Train: 5.3935 Val: 5.3125 Test: 5.8084  | ROC: Train: 0.835, Val: 0.856, Test: 0.812\n",
            "Epoch: 055 | LOSS: Train: 5.3491 Val: 5.2643 Test: 5.7516  | ROC: Train: 0.835, Val: 0.860, Test: 0.810\n",
            "Epoch: 056 | LOSS: Train: 5.3832 Val: 5.3539 Test: 5.7685  | ROC: Train: 0.828, Val: 0.853, Test: 0.810\n",
            "Epoch: 057 | LOSS: Train: 5.3350 Val: 5.4035 Test: 5.8681  | ROC: Train: 0.837, Val: 0.847, Test: 0.790\n",
            "Epoch: 058 | LOSS: Train: 5.2947 Val: 5.3264 Test: 5.7957  | ROC: Train: 0.838, Val: 0.866, Test: 0.819\n",
            "Epoch: 059 | LOSS: Train: 5.2538 Val: 5.2330 Test: 5.6720  | ROC: Train: 0.845, Val: 0.867, Test: 0.823\n",
            "Epoch: 060 | LOSS: Train: 5.3486 Val: 5.1858 Test: 5.6157  | ROC: Train: 0.835, Val: 0.863, Test: 0.820\n",
            "Epoch: 061 | LOSS: Train: 5.2091 Val: 5.2547 Test: 5.7406  | ROC: Train: 0.840, Val: 0.853, Test: 0.805\n",
            "Epoch: 062 | LOSS: Train: 5.3273 Val: 5.1266 Test: 5.6234  | ROC: Train: 0.837, Val: 0.873, Test: 0.820\n",
            "Epoch: 063 | LOSS: Train: 5.2882 Val: 5.2494 Test: 5.7061  | ROC: Train: 0.836, Val: 0.863, Test: 0.820\n",
            "Epoch: 064 | LOSS: Train: 5.2275 Val: 5.2257 Test: 5.7031  | ROC: Train: 0.845, Val: 0.871, Test: 0.828\n",
            "Epoch: 065 | LOSS: Train: 5.1728 Val: 5.2328 Test: 5.6803  | ROC: Train: 0.850, Val: 0.871, Test: 0.824\n",
            "Epoch: 066 | LOSS: Train: 5.1810 Val: 5.2097 Test: 5.7232  | ROC: Train: 0.849, Val: 0.857, Test: 0.812\n",
            "Epoch: 067 | LOSS: Train: 5.1691 Val: 5.0876 Test: 5.5841  | ROC: Train: 0.846, Val: 0.873, Test: 0.825\n",
            "Epoch: 068 | LOSS: Train: 5.1105 Val: 5.2039 Test: 5.6173  | ROC: Train: 0.850, Val: 0.863, Test: 0.828\n",
            "Epoch: 069 | LOSS: Train: 5.1273 Val: 5.1023 Test: 5.5076  | ROC: Train: 0.852, Val: 0.871, Test: 0.825\n",
            "Epoch: 070 | LOSS: Train: 5.1091 Val: 5.1512 Test: 5.5947  | ROC: Train: 0.852, Val: 0.868, Test: 0.814\n",
            "Epoch: 071 | LOSS: Train: 5.0841 Val: 4.9688 Test: 5.4038  | ROC: Train: 0.845, Val: 0.877, Test: 0.835\n",
            "Epoch: 072 | LOSS: Train: 5.0415 Val: 5.0042 Test: 5.4135  | ROC: Train: 0.858, Val: 0.875, Test: 0.829\n",
            "Epoch: 073 | LOSS: Train: 5.0019 Val: 4.9197 Test: 5.3218  | ROC: Train: 0.859, Val: 0.877, Test: 0.832\n",
            "Epoch: 074 | LOSS: Train: 4.9795 Val: 4.9687 Test: 5.4378  | ROC: Train: 0.858, Val: 0.876, Test: 0.833\n",
            "Epoch: 075 | LOSS: Train: 4.9541 Val: 5.0061 Test: 5.3884  | ROC: Train: 0.860, Val: 0.875, Test: 0.830\n",
            "Epoch: 076 | LOSS: Train: 4.9514 Val: 5.0244 Test: 5.4718  | ROC: Train: 0.859, Val: 0.873, Test: 0.829\n",
            "Epoch: 077 | LOSS: Train: 4.8959 Val: 4.8849 Test: 5.2874  | ROC: Train: 0.860, Val: 0.876, Test: 0.832\n",
            "Epoch: 078 | LOSS: Train: 4.9789 Val: 5.1053 Test: 5.5851  | ROC: Train: 0.855, Val: 0.881, Test: 0.829\n",
            "Epoch: 079 | LOSS: Train: 4.9443 Val: 4.9731 Test: 5.3980  | ROC: Train: 0.859, Val: 0.882, Test: 0.834\n",
            "Epoch: 080 | LOSS: Train: 4.8859 Val: 5.1425 Test: 5.5458  | ROC: Train: 0.862, Val: 0.855, Test: 0.817\n",
            "Epoch: 081 | LOSS: Train: 4.8885 Val: 4.8205 Test: 5.2499  | ROC: Train: 0.859, Val: 0.882, Test: 0.836\n",
            "Epoch: 082 | LOSS: Train: 4.8597 Val: 4.9854 Test: 5.4315  | ROC: Train: 0.859, Val: 0.872, Test: 0.826\n",
            "Epoch: 083 | LOSS: Train: 4.8302 Val: 4.9988 Test: 5.3898  | ROC: Train: 0.866, Val: 0.869, Test: 0.827\n",
            "Epoch: 084 | LOSS: Train: 4.8157 Val: 5.1258 Test: 5.5794  | ROC: Train: 0.861, Val: 0.868, Test: 0.830\n",
            "Epoch: 085 | LOSS: Train: 4.8753 Val: 4.8445 Test: 5.2462  | ROC: Train: 0.858, Val: 0.882, Test: 0.838\n",
            "Epoch: 086 | LOSS: Train: 4.8391 Val: 4.7791 Test: 5.1923  | ROC: Train: 0.865, Val: 0.874, Test: 0.831\n",
            "Epoch: 087 | LOSS: Train: 4.8070 Val: 4.8903 Test: 5.3109  | ROC: Train: 0.864, Val: 0.872, Test: 0.832\n",
            "Epoch: 088 | LOSS: Train: 4.8319 Val: 4.7603 Test: 5.2390  | ROC: Train: 0.862, Val: 0.887, Test: 0.834\n",
            "Epoch: 089 | LOSS: Train: 4.7335 Val: 4.8607 Test: 5.2900  | ROC: Train: 0.869, Val: 0.868, Test: 0.829\n",
            "Epoch: 090 | LOSS: Train: 4.7360 Val: 4.8461 Test: 5.1851  | ROC: Train: 0.869, Val: 0.865, Test: 0.832\n",
            "Epoch: 091 | LOSS: Train: 4.7923 Val: 4.9835 Test: 5.4313  | ROC: Train: 0.867, Val: 0.862, Test: 0.813\n",
            "Epoch: 092 | LOSS: Train: 4.8298 Val: 4.7555 Test: 5.2277  | ROC: Train: 0.864, Val: 0.880, Test: 0.832\n",
            "Epoch: 093 | LOSS: Train: 4.7439 Val: 4.7653 Test: 5.1532  | ROC: Train: 0.873, Val: 0.879, Test: 0.832\n",
            "Epoch: 094 | LOSS: Train: 4.7246 Val: 4.7438 Test: 5.2275  | ROC: Train: 0.865, Val: 0.876, Test: 0.827\n",
            "Epoch: 095 | LOSS: Train: 4.7475 Val: 4.8188 Test: 5.2969  | ROC: Train: 0.866, Val: 0.868, Test: 0.822\n",
            "Epoch: 096 | LOSS: Train: 4.6349 Val: 5.0108 Test: 5.4623  | ROC: Train: 0.879, Val: 0.871, Test: 0.832\n",
            "Epoch: 097 | LOSS: Train: 4.6720 Val: 4.8053 Test: 5.2512  | ROC: Train: 0.868, Val: 0.865, Test: 0.830\n",
            "Epoch: 098 | LOSS: Train: 4.7180 Val: 4.7522 Test: 5.2204  | ROC: Train: 0.867, Val: 0.888, Test: 0.841\n",
            "Epoch: 099 | LOSS: Train: 4.6821 Val: 4.7989 Test: 5.2534  | ROC: Train: 0.873, Val: 0.877, Test: 0.828\n",
            "Epoch: 100 | LOSS: Train: 4.7227 Val: 4.7266 Test: 5.1604  | ROC: Train: 0.866, Val: 0.882, Test: 0.838\n",
            "Epoch: 101 | LOSS: Train: 4.5406 Val: 4.6838 Test: 5.1620  | ROC: Train: 0.876, Val: 0.882, Test: 0.838\n",
            "Epoch: 102 | LOSS: Train: 4.6005 Val: 4.6817 Test: 5.2056  | ROC: Train: 0.876, Val: 0.879, Test: 0.830\n",
            "Epoch: 103 | LOSS: Train: 4.5647 Val: 4.8490 Test: 5.3357  | ROC: Train: 0.875, Val: 0.873, Test: 0.835\n",
            "Epoch: 104 | LOSS: Train: 4.5301 Val: 4.6545 Test: 5.1174  | ROC: Train: 0.881, Val: 0.879, Test: 0.842\n",
            "Epoch: 105 | LOSS: Train: 4.6064 Val: 4.6799 Test: 5.0702  | ROC: Train: 0.873, Val: 0.872, Test: 0.839\n",
            "Epoch: 106 | LOSS: Train: 4.5648 Val: 4.6798 Test: 5.1333  | ROC: Train: 0.869, Val: 0.883, Test: 0.839\n",
            "Epoch: 107 | LOSS: Train: 4.5062 Val: 4.9197 Test: 5.3756  | ROC: Train: 0.875, Val: 0.856, Test: 0.819\n",
            "Epoch: 108 | LOSS: Train: 4.5027 Val: 4.6405 Test: 5.1382  | ROC: Train: 0.875, Val: 0.879, Test: 0.840\n",
            "Epoch: 109 | LOSS: Train: 4.5342 Val: 4.6795 Test: 5.1375  | ROC: Train: 0.879, Val: 0.877, Test: 0.839\n",
            "Epoch: 110 | LOSS: Train: 4.5608 Val: 4.6650 Test: 5.0803  | ROC: Train: 0.877, Val: 0.880, Test: 0.837\n",
            "Epoch: 111 | LOSS: Train: 4.5745 Val: 4.7624 Test: 5.2616  | ROC: Train: 0.870, Val: 0.865, Test: 0.821\n",
            "Epoch: 112 | LOSS: Train: 4.5352 Val: 4.7062 Test: 5.1466  | ROC: Train: 0.874, Val: 0.873, Test: 0.832\n",
            "Epoch: 113 | LOSS: Train: 4.4689 Val: 4.6727 Test: 5.1324  | ROC: Train: 0.883, Val: 0.870, Test: 0.830\n",
            "Epoch: 114 | LOSS: Train: 4.5447 Val: 4.7124 Test: 5.2130  | ROC: Train: 0.867, Val: 0.876, Test: 0.831\n",
            "Epoch: 115 | LOSS: Train: 4.3882 Val: 4.7576 Test: 5.3152  | ROC: Train: 0.877, Val: 0.864, Test: 0.812\n",
            "Epoch: 116 | LOSS: Train: 4.4995 Val: 4.6881 Test: 5.2219  | ROC: Train: 0.883, Val: 0.870, Test: 0.824\n",
            "Epoch: 117 | LOSS: Train: 4.4487 Val: 4.6333 Test: 5.1174  | ROC: Train: 0.883, Val: 0.877, Test: 0.829\n",
            "Epoch: 118 | LOSS: Train: 4.4006 Val: 4.5796 Test: 5.0416  | ROC: Train: 0.884, Val: 0.872, Test: 0.836\n",
            "Epoch: 119 | LOSS: Train: 4.4273 Val: 4.6534 Test: 5.1378  | ROC: Train: 0.884, Val: 0.867, Test: 0.832\n",
            "Epoch: 120 | LOSS: Train: 4.5374 Val: 4.9656 Test: 5.5248  | ROC: Train: 0.885, Val: 0.870, Test: 0.822\n",
            "Epoch: 121 | LOSS: Train: 4.4668 Val: 4.5461 Test: 5.0503  | ROC: Train: 0.883, Val: 0.881, Test: 0.836\n",
            "Epoch: 122 | LOSS: Train: 4.3759 Val: 4.5982 Test: 5.0489  | ROC: Train: 0.884, Val: 0.880, Test: 0.839\n",
            "Epoch: 123 | LOSS: Train: 4.4839 Val: 4.6905 Test: 5.1525  | ROC: Train: 0.880, Val: 0.877, Test: 0.835\n",
            "Epoch: 124 | LOSS: Train: 4.4055 Val: 4.7165 Test: 5.1336  | ROC: Train: 0.887, Val: 0.888, Test: 0.842\n",
            "Epoch: 125 | LOSS: Train: 4.4399 Val: 4.5929 Test: 5.0576  | ROC: Train: 0.885, Val: 0.885, Test: 0.835\n",
            "Epoch: 126 | LOSS: Train: 4.3130 Val: 4.7052 Test: 5.2323  | ROC: Train: 0.890, Val: 0.872, Test: 0.820\n",
            "Epoch: 127 | LOSS: Train: 4.3592 Val: 4.7521 Test: 5.3037  | ROC: Train: 0.890, Val: 0.869, Test: 0.822\n",
            "Epoch: 128 | LOSS: Train: 4.3634 Val: 4.6718 Test: 5.1236  | ROC: Train: 0.884, Val: 0.882, Test: 0.836\n",
            "Epoch: 129 | LOSS: Train: 4.2806 Val: 4.5861 Test: 5.0667  | ROC: Train: 0.890, Val: 0.885, Test: 0.838\n",
            "Epoch: 130 | LOSS: Train: 4.3082 Val: 4.6934 Test: 5.2339  | ROC: Train: 0.889, Val: 0.871, Test: 0.814\n",
            "Epoch: 131 | LOSS: Train: 4.3130 Val: 4.6317 Test: 5.0606  | ROC: Train: 0.892, Val: 0.872, Test: 0.842\n",
            "Epoch: 132 | LOSS: Train: 4.3271 Val: 4.6821 Test: 5.1207  | ROC: Train: 0.883, Val: 0.873, Test: 0.840\n",
            "Epoch: 133 | LOSS: Train: 4.3003 Val: 4.6594 Test: 5.1938  | ROC: Train: 0.892, Val: 0.870, Test: 0.822\n",
            "Epoch: 134 | LOSS: Train: 4.2760 Val: 4.6767 Test: 5.0879  | ROC: Train: 0.887, Val: 0.872, Test: 0.835\n",
            "Epoch: 135 | LOSS: Train: 4.2487 Val: 4.5352 Test: 5.0493  | ROC: Train: 0.891, Val: 0.879, Test: 0.829\n",
            "Epoch: 136 | LOSS: Train: 4.2290 Val: 4.5692 Test: 5.0513  | ROC: Train: 0.890, Val: 0.880, Test: 0.831\n",
            "Epoch: 137 | LOSS: Train: 4.1971 Val: 4.7059 Test: 5.2058  | ROC: Train: 0.896, Val: 0.886, Test: 0.834\n",
            "Epoch: 138 | LOSS: Train: 4.2952 Val: 4.7364 Test: 5.1761  | ROC: Train: 0.893, Val: 0.881, Test: 0.827\n",
            "Epoch: 139 | LOSS: Train: 4.2147 Val: 4.4969 Test: 5.0280  | ROC: Train: 0.895, Val: 0.887, Test: 0.838\n",
            "Epoch: 140 | LOSS: Train: 4.0757 Val: 4.6707 Test: 5.2195  | ROC: Train: 0.900, Val: 0.879, Test: 0.825\n",
            "Epoch: 141 | LOSS: Train: 4.2418 Val: 4.5494 Test: 5.0153  | ROC: Train: 0.896, Val: 0.868, Test: 0.831\n",
            "Epoch: 142 | LOSS: Train: 4.1886 Val: 4.5802 Test: 4.9954  | ROC: Train: 0.896, Val: 0.882, Test: 0.839\n",
            "Epoch: 143 | LOSS: Train: 4.1775 Val: 4.5727 Test: 5.0873  | ROC: Train: 0.897, Val: 0.868, Test: 0.826\n",
            "Epoch: 144 | LOSS: Train: 4.2466 Val: 4.6172 Test: 5.1486  | ROC: Train: 0.895, Val: 0.869, Test: 0.825\n",
            "Epoch: 145 | LOSS: Train: 4.2202 Val: 4.5345 Test: 5.0451  | ROC: Train: 0.897, Val: 0.876, Test: 0.831\n",
            "Epoch: 146 | LOSS: Train: 4.1375 Val: 4.6220 Test: 5.1338  | ROC: Train: 0.895, Val: 0.887, Test: 0.832\n",
            "Epoch: 147 | LOSS: Train: 4.1985 Val: 4.6105 Test: 5.1196  | ROC: Train: 0.894, Val: 0.877, Test: 0.824\n",
            "Epoch: 148 | LOSS: Train: 4.1436 Val: 4.4411 Test: 5.0262  | ROC: Train: 0.895, Val: 0.888, Test: 0.834\n",
            "Epoch: 149 | LOSS: Train: 4.2096 Val: 4.5405 Test: 5.1155  | ROC: Train: 0.893, Val: 0.882, Test: 0.824\n",
            "Epoch: 150 | LOSS: Train: 4.1126 Val: 4.6740 Test: 5.1011  | ROC: Train: 0.899, Val: 0.871, Test: 0.828\n",
            "Epoch: 151 | LOSS: Train: 4.0963 Val: 4.4946 Test: 5.0214  | ROC: Train: 0.898, Val: 0.888, Test: 0.833\n",
            "Epoch: 152 | LOSS: Train: 4.0858 Val: 4.6138 Test: 5.1528  | ROC: Train: 0.899, Val: 0.887, Test: 0.823\n",
            "Epoch: 153 | LOSS: Train: 4.2159 Val: 4.6424 Test: 5.1216  | ROC: Train: 0.891, Val: 0.888, Test: 0.833\n",
            "Epoch: 154 | LOSS: Train: 4.0837 Val: 4.5370 Test: 5.1141  | ROC: Train: 0.904, Val: 0.880, Test: 0.826\n",
            "Epoch: 155 | LOSS: Train: 4.0887 Val: 4.4139 Test: 5.0476  | ROC: Train: 0.905, Val: 0.888, Test: 0.833\n",
            "Epoch: 156 | LOSS: Train: 4.0634 Val: 4.5192 Test: 5.0603  | ROC: Train: 0.899, Val: 0.888, Test: 0.824\n",
            "Epoch: 157 | LOSS: Train: 4.0932 Val: 4.5186 Test: 5.1082  | ROC: Train: 0.896, Val: 0.887, Test: 0.830\n",
            "Epoch: 158 | LOSS: Train: 4.0785 Val: 4.6286 Test: 5.2826  | ROC: Train: 0.902, Val: 0.880, Test: 0.808\n",
            "Epoch: 159 | LOSS: Train: 3.9940 Val: 4.6420 Test: 5.2063  | ROC: Train: 0.906, Val: 0.866, Test: 0.807\n",
            "Epoch: 160 | LOSS: Train: 3.9290 Val: 4.5769 Test: 5.2205  | ROC: Train: 0.909, Val: 0.884, Test: 0.821\n",
            "Epoch: 161 | LOSS: Train: 4.0188 Val: 4.5639 Test: 5.1514  | ROC: Train: 0.908, Val: 0.885, Test: 0.825\n",
            "Epoch: 162 | LOSS: Train: 4.0622 Val: 4.5419 Test: 5.0872  | ROC: Train: 0.904, Val: 0.882, Test: 0.826\n",
            "Epoch: 163 | LOSS: Train: 4.0533 Val: 4.5372 Test: 5.1432  | ROC: Train: 0.903, Val: 0.887, Test: 0.837\n",
            "Epoch: 164 | LOSS: Train: 3.9265 Val: 4.8662 Test: 5.5060  | ROC: Train: 0.912, Val: 0.853, Test: 0.804\n",
            "Epoch: 165 | LOSS: Train: 3.9755 Val: 4.5752 Test: 5.0689  | ROC: Train: 0.909, Val: 0.877, Test: 0.825\n",
            "Epoch: 166 | LOSS: Train: 3.9568 Val: 4.5486 Test: 5.3179  | ROC: Train: 0.908, Val: 0.889, Test: 0.821\n",
            "Epoch: 167 | LOSS: Train: 3.9317 Val: 4.6462 Test: 5.0415  | ROC: Train: 0.908, Val: 0.881, Test: 0.826\n",
            "Epoch: 168 | LOSS: Train: 3.9939 Val: 4.6997 Test: 5.1475  | ROC: Train: 0.908, Val: 0.880, Test: 0.821\n",
            "Epoch: 169 | LOSS: Train: 4.0163 Val: 4.6187 Test: 5.1197  | ROC: Train: 0.906, Val: 0.872, Test: 0.817\n",
            "Epoch: 170 | LOSS: Train: 3.9111 Val: 4.4271 Test: 5.0812  | ROC: Train: 0.910, Val: 0.883, Test: 0.824\n",
            "Epoch: 171 | LOSS: Train: 3.8715 Val: 4.6069 Test: 5.1840  | ROC: Train: 0.909, Val: 0.885, Test: 0.810\n",
            "Epoch: 172 | LOSS: Train: 3.9444 Val: 4.6854 Test: 5.1748  | ROC: Train: 0.908, Val: 0.872, Test: 0.813\n",
            "Epoch: 173 | LOSS: Train: 3.9565 Val: 4.6681 Test: 5.2874  | ROC: Train: 0.904, Val: 0.866, Test: 0.805\n",
            "Epoch: 174 | LOSS: Train: 3.9587 Val: 4.6139 Test: 5.1810  | ROC: Train: 0.910, Val: 0.885, Test: 0.821\n",
            "Epoch: 175 | LOSS: Train: 3.8673 Val: 4.7164 Test: 5.3104  | ROC: Train: 0.912, Val: 0.882, Test: 0.813\n",
            "Epoch: 176 | LOSS: Train: 3.8903 Val: 4.5962 Test: 5.1959  | ROC: Train: 0.910, Val: 0.874, Test: 0.806\n",
            "Epoch: 177 | LOSS: Train: 3.8728 Val: 4.5548 Test: 5.2056  | ROC: Train: 0.909, Val: 0.886, Test: 0.814\n",
            "Epoch: 178 | LOSS: Train: 3.9439 Val: 4.5951 Test: 5.2049  | ROC: Train: 0.909, Val: 0.886, Test: 0.813\n",
            "Epoch: 179 | LOSS: Train: 3.8455 Val: 4.6524 Test: 5.2946  | ROC: Train: 0.920, Val: 0.887, Test: 0.812\n",
            "Epoch: 180 | LOSS: Train: 3.8222 Val: 4.6235 Test: 5.2464  | ROC: Train: 0.918, Val: 0.878, Test: 0.814\n",
            "Epoch: 181 | LOSS: Train: 3.7867 Val: 4.5906 Test: 5.1391  | ROC: Train: 0.915, Val: 0.883, Test: 0.821\n",
            "Epoch: 182 | LOSS: Train: 3.8350 Val: 4.5490 Test: 5.0899  | ROC: Train: 0.917, Val: 0.873, Test: 0.823\n",
            "Epoch: 183 | LOSS: Train: 3.7924 Val: 4.5404 Test: 5.0736  | ROC: Train: 0.917, Val: 0.882, Test: 0.833\n",
            "Epoch: 184 | LOSS: Train: 3.8794 Val: 4.5808 Test: 5.2471  | ROC: Train: 0.911, Val: 0.863, Test: 0.813\n",
            "Epoch: 185 | LOSS: Train: 3.7783 Val: 4.5089 Test: 5.2108  | ROC: Train: 0.918, Val: 0.882, Test: 0.818\n",
            "Epoch: 186 | LOSS: Train: 3.9021 Val: 4.7574 Test: 5.4410  | ROC: Train: 0.911, Val: 0.851, Test: 0.799\n",
            "Epoch: 187 | LOSS: Train: 3.8437 Val: 4.5280 Test: 5.1339  | ROC: Train: 0.917, Val: 0.879, Test: 0.828\n",
            "Epoch: 188 | LOSS: Train: 3.7502 Val: 4.7101 Test: 5.2322  | ROC: Train: 0.916, Val: 0.867, Test: 0.811\n",
            "Epoch: 189 | LOSS: Train: 3.7337 Val: 4.5072 Test: 5.2570  | ROC: Train: 0.924, Val: 0.882, Test: 0.816\n",
            "Epoch: 190 | LOSS: Train: 3.7911 Val: 4.5466 Test: 5.1690  | ROC: Train: 0.915, Val: 0.884, Test: 0.827\n",
            "Epoch: 191 | LOSS: Train: 3.7229 Val: 4.5181 Test: 5.1864  | ROC: Train: 0.923, Val: 0.869, Test: 0.816\n",
            "Epoch: 192 | LOSS: Train: 3.8083 Val: 4.5986 Test: 5.2103  | ROC: Train: 0.916, Val: 0.855, Test: 0.808\n",
            "Epoch: 193 | LOSS: Train: 3.7820 Val: 4.6120 Test: 5.2555  | ROC: Train: 0.914, Val: 0.871, Test: 0.814\n",
            "Epoch: 194 | LOSS: Train: 3.7337 Val: 4.5449 Test: 5.2969  | ROC: Train: 0.917, Val: 0.859, Test: 0.811\n",
            "Epoch: 195 | LOSS: Train: 3.7316 Val: 4.6175 Test: 5.2116  | ROC: Train: 0.918, Val: 0.874, Test: 0.824\n",
            "Epoch: 196 | LOSS: Train: 3.6979 Val: 4.5979 Test: 5.1018  | ROC: Train: 0.923, Val: 0.861, Test: 0.818\n",
            "Epoch: 197 | LOSS: Train: 3.6945 Val: 4.5550 Test: 5.2750  | ROC: Train: 0.925, Val: 0.894, Test: 0.820\n",
            "Epoch: 198 | LOSS: Train: 3.6703 Val: 4.6554 Test: 5.2227  | ROC: Train: 0.925, Val: 0.873, Test: 0.826\n",
            "Epoch: 199 | LOSS: Train: 3.7032 Val: 4.5512 Test: 5.2337  | ROC: Train: 0.921, Val: 0.881, Test: 0.829\n"
          ]
        }
      ],
      "source": [
        "# Train the Attentive FP\n",
        "\n",
        "history = []\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    train_loss, train_roc = train()   #  train loss and roc with dropout\n",
        "    val_loss, val_roc = test(val_loader)\n",
        "    test_loss, test_roc = test(test_loader)\n",
        "    # train_loss, train_roc = test(train_loader)  # compute loss without dropout\n",
        "    print(f'Epoch: {epoch:03d} | LOSS: Train: {train_loss:.4f} Val: {val_loss:.4f} Test: {test_loss:.4f}  | ROC: Train: {train_roc:.3f}, Val: {val_roc:.3f}, '\n",
        "          f'Test: {test_roc:.3f}')\n",
        "    history.append({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss,'test_loss': test_loss, 'train_roc': train_roc, 'val_roc': val_roc, 'test_roc': test_roc})\n",
        "    \n",
        "    # Uncomment to save the model\n",
        "    # Save the model in a dictionary with optimizer state for training \n",
        "    # continuation (first) or save the model for inference only (second)\n",
        "\n",
        "    # PATH = f\"/content/drive/MyDrive/AI/covid-1/model250/covid-250-l4-t3--1-epoch{epoch:03d}.pt\"\n",
        "    # torch.save({\n",
        "    #         'epoch': epoch,\n",
        "    #         'model_state_dict': model.state_dict(),\n",
        "    #         'optimizer_state_dict': optimizer.state_dict(),\n",
        "    #         'loss': train_loss,\n",
        "    #         }, PATH)\n",
        "    # torch.save(model,  PATH.strip(\".pt\") + \"model1.pt\" )"
      ],
      "id": "-NWhQZfCpNfy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the GraphConv Network"
      ],
      "metadata": {
        "id": "IiFCVp9RWZ_f"
      },
      "id": "IiFCVp9RWZ_f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "353d6a95-c408-4f29-b83a-1f01c376ff63",
        "outputId": "f749ef39-de6a-4b08-a870-ac4b679f1503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): GraphConv(44, 200)\n",
            "  (conv2): GraphConv(200, 200)\n",
            "  (conv3): GraphConv(200, 200)\n",
            "  (lin): Linear(in_features=200, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# !pip install class-resolver  # Might be needed depending on the torch version\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GraphConv, GraphSAGE\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(Net, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GraphConv(44, hidden_channels)\n",
        "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        # x = x.sigmoid()\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = Net(hidden_channels=200)\n",
        "print(model)"
      ],
      "id": "353d6a95-c408-4f29-b83a-1f01c376ff63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg316w-QyLbz",
        "outputId": "f88e2dc7-3df2-4627-b0c7-452b7696e9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001 | LOSS: Train: 13.7756 Val: 5.7125 Test: 5.7591  | ROC: Train: 0.487, Val: 0.440, Test: 0.442\n",
            "Epoch: 002 | LOSS: Train: 5.5904 Val: 5.5062 Test: 5.5534  | ROC: Train: 0.489, Val: 0.459, Test: 0.460\n",
            "Epoch: 003 | LOSS: Train: 5.4968 Val: 5.3784 Test: 5.4370  | ROC: Train: 0.522, Val: 0.530, Test: 0.540\n",
            "Epoch: 004 | LOSS: Train: 5.3182 Val: 5.2966 Test: 5.3624  | ROC: Train: 0.629, Val: 0.626, Test: 0.614\n",
            "Epoch: 005 | LOSS: Train: 5.2299 Val: 5.2272 Test: 5.2941  | ROC: Train: 0.672, Val: 0.663, Test: 0.649\n",
            "Epoch: 006 | LOSS: Train: 5.1241 Val: 5.2195 Test: 5.2895  | ROC: Train: 0.717, Val: 0.682, Test: 0.676\n",
            "Epoch: 007 | LOSS: Train: 5.0542 Val: 5.1010 Test: 5.1778  | ROC: Train: 0.732, Val: 0.704, Test: 0.700\n",
            "Epoch: 008 | LOSS: Train: 4.9864 Val: 5.0393 Test: 5.1258  | ROC: Train: 0.744, Val: 0.718, Test: 0.718\n",
            "Epoch: 009 | LOSS: Train: 4.9270 Val: 4.9738 Test: 5.0805  | ROC: Train: 0.752, Val: 0.727, Test: 0.727\n",
            "Epoch: 010 | LOSS: Train: 4.8398 Val: 5.0000 Test: 5.1380  | ROC: Train: 0.769, Val: 0.736, Test: 0.735\n",
            "Epoch: 011 | LOSS: Train: 4.7847 Val: 4.8676 Test: 5.0167  | ROC: Train: 0.782, Val: 0.745, Test: 0.744\n",
            "Epoch: 012 | LOSS: Train: 4.7632 Val: 4.9060 Test: 5.0807  | ROC: Train: 0.780, Val: 0.749, Test: 0.747\n",
            "Epoch: 013 | LOSS: Train: 4.7680 Val: 4.8295 Test: 5.0178  | ROC: Train: 0.775, Val: 0.760, Test: 0.754\n",
            "Epoch: 014 | LOSS: Train: 4.6878 Val: 4.8701 Test: 5.0784  | ROC: Train: 0.790, Val: 0.755, Test: 0.751\n",
            "Epoch: 015 | LOSS: Train: 4.6538 Val: 4.8704 Test: 5.0869  | ROC: Train: 0.797, Val: 0.760, Test: 0.754\n",
            "Epoch: 016 | LOSS: Train: 4.6430 Val: 4.7599 Test: 4.9757  | ROC: Train: 0.796, Val: 0.773, Test: 0.761\n",
            "Epoch: 017 | LOSS: Train: 4.6186 Val: 4.6707 Test: 4.9001  | ROC: Train: 0.799, Val: 0.772, Test: 0.763\n",
            "Epoch: 018 | LOSS: Train: 4.5536 Val: 4.6938 Test: 4.9639  | ROC: Train: 0.808, Val: 0.764, Test: 0.747\n",
            "Epoch: 019 | LOSS: Train: 4.5484 Val: 4.6985 Test: 4.9431  | ROC: Train: 0.806, Val: 0.782, Test: 0.766\n"
          ]
        }
      ],
      "source": [
        "WEIGHT = 230\n",
        "print_every = 0\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=10**-3.5,\n",
        "                             weight_decay=10**-2.9)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    \n",
        "    total_pred = [] \n",
        "    total_y = []\n",
        "    total_loss = total_examples = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        out = out.sigmoid()\n",
        "\n",
        "        weight = torch.tensor([WEIGHT], dtype=torch.float)\n",
        "        loss = F.binary_cross_entropy(out, data.y, weight=weight.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        total_examples += data.num_graphs\n",
        "        total_pred.append(out.detach().cpu().numpy())\n",
        "        total_y.append(data.y.detach().cpu().numpy())\n",
        "        \n",
        "        if print_every != 0 and i % print_every == 0:\n",
        "            val_loss, val_roc = test(val_loader)\n",
        "            test_loss, test_roc = test(test_loader)\n",
        "            fpr, tpr, threshold = metrics.roc_curve(np.concatenate(total_y, 0), np.concatenate(total_pred, 0))\n",
        "            roc_auc = metrics.auc(fpr, tpr)\n",
        "            print(f'Epoch: {epoch:03d} | LOSS: Train: {total_loss / total_examples:.4f} Val: {val_loss:.4f} Test: {test_loss:.4f}  | ROC: Train: {roc_auc:.3f} Val: {val_roc:.3f}, '\n",
        "                  f'Test: {test_roc:.3f}')\n",
        "            \n",
        "        \n",
        "    fpr, tpr, threshold = metrics.roc_curve(np.concatenate(total_y, 0), np.concatenate(total_pred, 0))\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return total_loss / total_examples, roc_auc\n",
        " \n",
        "\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "    total_pred = [] #np.array([], dtype=float)\n",
        "    total_y = [] # np.array([], dtype=float)\n",
        "    \n",
        "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index,  data.batch)  \n",
        "        pred = torch.sigmoid(out)  #  out.sigmoid()\n",
        "        total_pred.append(pred.detach().cpu().numpy())\n",
        "        total_y.append(data.y.detach().cpu().numpy())\n",
        "\n",
        "            \n",
        "        weight = torch.tensor([WEIGHT], dtype=torch.float)\n",
        "        loss_test = F.binary_cross_entropy(pred, data.y, weight=weight.to(device))\n",
        "        total_loss += float(loss_test)  * data.num_graphs\n",
        "        total_examples += data.num_graphs\n",
        "\n",
        "    fpr, tpr, threshold = metrics.roc_curve(np.concatenate(total_y, 0), np.concatenate(total_pred, 0))\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return total_loss / total_examples , roc_auc\n",
        " \n",
        "for epoch in range(1, 20):\n",
        "    train_loss, train_roc = train()\n",
        "    val_loss, val_roc = test(val_loader)\n",
        "    test_loss, test_roc = test(test_loader)\n",
        "    # train_loss, train_roc = test(train_loader)\n",
        "    print(f'Epoch: {epoch:03d} | LOSS: Train: {train_loss:.4f} Val: {val_loss:.4f} Test: {test_loss:.4f}  | ROC: Train: {train_roc:.3f}, Val: {val_roc:.3f}, '\n",
        "          f'Test: {test_roc:.3f}')"
      ],
      "id": "wg316w-QyLbz"
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 180):\n",
        "    train_loss, train_roc = train()\n",
        "    val_loss, val_roc = test(val_loader)\n",
        "    test_loss, test_roc = test(test_loader)\n",
        "    # train_loss, train_roc = test(train_loader)\n",
        "    print(f'Epoch: {epoch:03d} | LOSS: Train: {train_loss:.4f} Val: {val_loss:.4f} Test: {test_loss:.4f}  | ROC: Train: {train_roc:.3f}, Val: {val_roc:.3f}, '\n",
        "          f'Test: {test_roc:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRreLXYlYqis",
        "outputId": "f06a7b83-6db4-4b99-e05a-34a57c0ce10e"
      },
      "id": "BRreLXYlYqis",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001 | LOSS: Train: 4.5085 Val: 4.6167 Test: 4.8653  | ROC: Train: 0.814, Val: 0.784, Test: 0.768\n",
            "Epoch: 002 | LOSS: Train: 4.4815 Val: 4.5844 Test: 4.8378  | ROC: Train: 0.818, Val: 0.779, Test: 0.767\n",
            "Epoch: 003 | LOSS: Train: 4.4854 Val: 4.6032 Test: 4.8825  | ROC: Train: 0.818, Val: 0.782, Test: 0.765\n",
            "Epoch: 004 | LOSS: Train: 4.4725 Val: 4.7274 Test: 5.0042  | ROC: Train: 0.815, Val: 0.793, Test: 0.771\n",
            "Epoch: 005 | LOSS: Train: 4.4776 Val: 4.5497 Test: 4.8346  | ROC: Train: 0.814, Val: 0.791, Test: 0.768\n",
            "Epoch: 006 | LOSS: Train: 4.4796 Val: 4.5761 Test: 4.8700  | ROC: Train: 0.815, Val: 0.794, Test: 0.769\n",
            "Epoch: 007 | LOSS: Train: 4.4421 Val: 4.5141 Test: 4.8180  | ROC: Train: 0.816, Val: 0.790, Test: 0.766\n",
            "Epoch: 008 | LOSS: Train: 4.3750 Val: 4.5394 Test: 4.8269  | ROC: Train: 0.829, Val: 0.796, Test: 0.770\n",
            "Epoch: 009 | LOSS: Train: 4.3941 Val: 4.5273 Test: 4.8296  | ROC: Train: 0.821, Val: 0.793, Test: 0.771\n",
            "Epoch: 010 | LOSS: Train: 4.3764 Val: 4.5068 Test: 4.8049  | ROC: Train: 0.824, Val: 0.800, Test: 0.774\n",
            "Epoch: 011 | LOSS: Train: 4.4043 Val: 4.4665 Test: 4.8013  | ROC: Train: 0.820, Val: 0.797, Test: 0.766\n",
            "Epoch: 012 | LOSS: Train: 4.3212 Val: 4.5888 Test: 4.8603  | ROC: Train: 0.830, Val: 0.804, Test: 0.776\n",
            "Epoch: 013 | LOSS: Train: 4.3654 Val: 4.4970 Test: 4.8433  | ROC: Train: 0.822, Val: 0.794, Test: 0.765\n",
            "Epoch: 014 | LOSS: Train: 4.2885 Val: 4.4129 Test: 4.7502  | ROC: Train: 0.832, Val: 0.805, Test: 0.771\n",
            "Epoch: 015 | LOSS: Train: 4.3000 Val: 4.4158 Test: 4.7276  | ROC: Train: 0.831, Val: 0.815, Test: 0.778\n",
            "Epoch: 016 | LOSS: Train: 4.2634 Val: 4.4042 Test: 4.7146  | ROC: Train: 0.836, Val: 0.806, Test: 0.777\n",
            "Epoch: 017 | LOSS: Train: 4.2738 Val: 4.5002 Test: 4.8474  | ROC: Train: 0.833, Val: 0.811, Test: 0.774\n",
            "Epoch: 018 | LOSS: Train: 4.2930 Val: 4.5301 Test: 4.8807  | ROC: Train: 0.833, Val: 0.809, Test: 0.773\n",
            "Epoch: 019 | LOSS: Train: 4.2453 Val: 4.3868 Test: 4.7205  | ROC: Train: 0.836, Val: 0.806, Test: 0.773\n",
            "Epoch: 020 | LOSS: Train: 4.2204 Val: 4.3818 Test: 4.7149  | ROC: Train: 0.836, Val: 0.816, Test: 0.776\n",
            "Epoch: 021 | LOSS: Train: 4.1970 Val: 4.5696 Test: 4.9380  | ROC: Train: 0.839, Val: 0.818, Test: 0.779\n",
            "Epoch: 022 | LOSS: Train: 4.1625 Val: 4.3940 Test: 4.7260  | ROC: Train: 0.844, Val: 0.823, Test: 0.779\n",
            "Epoch: 023 | LOSS: Train: 4.1480 Val: 4.3783 Test: 4.7256  | ROC: Train: 0.845, Val: 0.818, Test: 0.777\n",
            "Epoch: 024 | LOSS: Train: 4.1364 Val: 4.3113 Test: 4.6939  | ROC: Train: 0.844, Val: 0.819, Test: 0.773\n",
            "Epoch: 025 | LOSS: Train: 4.1459 Val: 4.3245 Test: 4.6595  | ROC: Train: 0.842, Val: 0.822, Test: 0.778\n",
            "Epoch: 026 | LOSS: Train: 4.1360 Val: 4.2775 Test: 4.6552  | ROC: Train: 0.845, Val: 0.818, Test: 0.773\n",
            "Epoch: 027 | LOSS: Train: 4.1279 Val: 4.2906 Test: 4.6629  | ROC: Train: 0.843, Val: 0.811, Test: 0.772\n",
            "Epoch: 028 | LOSS: Train: 4.1033 Val: 4.3495 Test: 4.7443  | ROC: Train: 0.848, Val: 0.812, Test: 0.772\n",
            "Epoch: 029 | LOSS: Train: 4.0845 Val: 4.4203 Test: 4.7602  | ROC: Train: 0.846, Val: 0.827, Test: 0.776\n",
            "Epoch: 030 | LOSS: Train: 4.0624 Val: 4.2416 Test: 4.6393  | ROC: Train: 0.851, Val: 0.820, Test: 0.771\n",
            "Epoch: 031 | LOSS: Train: 4.0465 Val: 4.3235 Test: 4.7421  | ROC: Train: 0.852, Val: 0.823, Test: 0.773\n",
            "Epoch: 032 | LOSS: Train: 4.0714 Val: 4.2721 Test: 4.6659  | ROC: Train: 0.849, Val: 0.817, Test: 0.772\n",
            "Epoch: 033 | LOSS: Train: 4.0063 Val: 4.2320 Test: 4.6025  | ROC: Train: 0.859, Val: 0.818, Test: 0.777\n",
            "Epoch: 034 | LOSS: Train: 4.0054 Val: 4.7883 Test: 5.2413  | ROC: Train: 0.855, Val: 0.813, Test: 0.772\n",
            "Epoch: 035 | LOSS: Train: 4.0657 Val: 4.3042 Test: 4.6583  | ROC: Train: 0.849, Val: 0.826, Test: 0.779\n",
            "Epoch: 036 | LOSS: Train: 3.9773 Val: 4.1826 Test: 4.5742  | ROC: Train: 0.856, Val: 0.826, Test: 0.774\n",
            "Epoch: 037 | LOSS: Train: 3.9879 Val: 4.1856 Test: 4.5747  | ROC: Train: 0.857, Val: 0.823, Test: 0.772\n",
            "Epoch: 038 | LOSS: Train: 3.9430 Val: 4.1845 Test: 4.6155  | ROC: Train: 0.862, Val: 0.824, Test: 0.776\n",
            "Epoch: 039 | LOSS: Train: 3.9417 Val: 4.3524 Test: 4.7338  | ROC: Train: 0.860, Val: 0.827, Test: 0.777\n",
            "Epoch: 040 | LOSS: Train: 3.9111 Val: 4.1503 Test: 4.5576  | ROC: Train: 0.861, Val: 0.826, Test: 0.774\n",
            "Epoch: 041 | LOSS: Train: 3.9233 Val: 4.1203 Test: 4.5288  | ROC: Train: 0.858, Val: 0.835, Test: 0.779\n",
            "Epoch: 042 | LOSS: Train: 3.9412 Val: 4.2082 Test: 4.6324  | ROC: Train: 0.856, Val: 0.827, Test: 0.777\n",
            "Epoch: 043 | LOSS: Train: 4.0258 Val: 4.5207 Test: 5.0125  | ROC: Train: 0.847, Val: 0.823, Test: 0.773\n",
            "Epoch: 044 | LOSS: Train: 3.9012 Val: 4.1411 Test: 4.5515  | ROC: Train: 0.862, Val: 0.824, Test: 0.774\n",
            "Epoch: 045 | LOSS: Train: 3.8498 Val: 4.3791 Test: 4.7508  | ROC: Train: 0.868, Val: 0.836, Test: 0.780\n",
            "Epoch: 046 | LOSS: Train: 3.8865 Val: 4.1656 Test: 4.6150  | ROC: Train: 0.865, Val: 0.826, Test: 0.776\n",
            "Epoch: 047 | LOSS: Train: 3.8485 Val: 4.0883 Test: 4.5205  | ROC: Train: 0.867, Val: 0.838, Test: 0.773\n",
            "Epoch: 048 | LOSS: Train: 3.8295 Val: 4.1092 Test: 4.5100  | ROC: Train: 0.867, Val: 0.832, Test: 0.779\n",
            "Epoch: 049 | LOSS: Train: 3.8255 Val: 4.1105 Test: 4.5204  | ROC: Train: 0.870, Val: 0.835, Test: 0.778\n",
            "Epoch: 050 | LOSS: Train: 3.8260 Val: 4.0433 Test: 4.5291  | ROC: Train: 0.869, Val: 0.843, Test: 0.775\n",
            "Epoch: 051 | LOSS: Train: 3.7807 Val: 4.0897 Test: 4.5573  | ROC: Train: 0.873, Val: 0.838, Test: 0.772\n",
            "Epoch: 052 | LOSS: Train: 3.8150 Val: 4.1494 Test: 4.6205  | ROC: Train: 0.868, Val: 0.834, Test: 0.776\n",
            "Epoch: 053 | LOSS: Train: 3.7745 Val: 4.1246 Test: 4.5258  | ROC: Train: 0.871, Val: 0.835, Test: 0.779\n",
            "Epoch: 054 | LOSS: Train: 3.7559 Val: 4.1401 Test: 4.6186  | ROC: Train: 0.874, Val: 0.832, Test: 0.776\n",
            "Epoch: 055 | LOSS: Train: 3.7393 Val: 4.0895 Test: 4.5357  | ROC: Train: 0.875, Val: 0.837, Test: 0.776\n",
            "Epoch: 056 | LOSS: Train: 3.8097 Val: 4.0361 Test: 4.5089  | ROC: Train: 0.872, Val: 0.835, Test: 0.776\n",
            "Epoch: 057 | LOSS: Train: 3.7622 Val: 4.0610 Test: 4.5806  | ROC: Train: 0.874, Val: 0.838, Test: 0.774\n",
            "Epoch: 058 | LOSS: Train: 3.7371 Val: 4.4872 Test: 4.8636  | ROC: Train: 0.878, Val: 0.841, Test: 0.782\n",
            "Epoch: 059 | LOSS: Train: 3.8046 Val: 4.0881 Test: 4.5574  | ROC: Train: 0.871, Val: 0.834, Test: 0.780\n",
            "Epoch: 060 | LOSS: Train: 3.6931 Val: 4.0525 Test: 4.5280  | ROC: Train: 0.878, Val: 0.842, Test: 0.779\n",
            "Epoch: 061 | LOSS: Train: 3.7130 Val: 4.0975 Test: 4.5827  | ROC: Train: 0.878, Val: 0.834, Test: 0.777\n",
            "Epoch: 062 | LOSS: Train: 3.7216 Val: 4.0294 Test: 4.4882  | ROC: Train: 0.878, Val: 0.842, Test: 0.780\n",
            "Epoch: 063 | LOSS: Train: 3.7651 Val: 4.1059 Test: 4.5787  | ROC: Train: 0.872, Val: 0.838, Test: 0.777\n",
            "Epoch: 064 | LOSS: Train: 3.6684 Val: 4.0131 Test: 4.4895  | ROC: Train: 0.881, Val: 0.835, Test: 0.778\n",
            "Epoch: 065 | LOSS: Train: 3.6695 Val: 4.1220 Test: 4.6181  | ROC: Train: 0.883, Val: 0.832, Test: 0.777\n",
            "Epoch: 066 | LOSS: Train: 3.6385 Val: 4.2597 Test: 4.6501  | ROC: Train: 0.884, Val: 0.846, Test: 0.788\n",
            "Epoch: 067 | LOSS: Train: 3.6780 Val: 3.9982 Test: 4.4522  | ROC: Train: 0.880, Val: 0.836, Test: 0.782\n",
            "Epoch: 068 | LOSS: Train: 3.5779 Val: 3.9895 Test: 4.4620  | ROC: Train: 0.889, Val: 0.845, Test: 0.782\n",
            "Epoch: 069 | LOSS: Train: 3.6414 Val: 4.1877 Test: 4.6258  | ROC: Train: 0.885, Val: 0.848, Test: 0.782\n",
            "Epoch: 070 | LOSS: Train: 3.7087 Val: 4.1859 Test: 4.6989  | ROC: Train: 0.874, Val: 0.830, Test: 0.773\n",
            "Epoch: 071 | LOSS: Train: 3.6459 Val: 4.3083 Test: 4.7511  | ROC: Train: 0.883, Val: 0.846, Test: 0.785\n",
            "Epoch: 072 | LOSS: Train: 3.6726 Val: 4.0460 Test: 4.5428  | ROC: Train: 0.879, Val: 0.836, Test: 0.780\n",
            "Epoch: 073 | LOSS: Train: 3.5970 Val: 3.9824 Test: 4.4330  | ROC: Train: 0.890, Val: 0.843, Test: 0.779\n",
            "Epoch: 074 | LOSS: Train: 3.5967 Val: 3.9669 Test: 4.4655  | ROC: Train: 0.887, Val: 0.835, Test: 0.774\n",
            "Epoch: 075 | LOSS: Train: 3.5966 Val: 4.0807 Test: 4.5394  | ROC: Train: 0.888, Val: 0.841, Test: 0.781\n",
            "Epoch: 076 | LOSS: Train: 3.5501 Val: 3.9424 Test: 4.4352  | ROC: Train: 0.893, Val: 0.845, Test: 0.780\n",
            "Epoch: 077 | LOSS: Train: 3.5550 Val: 3.9743 Test: 4.4537  | ROC: Train: 0.891, Val: 0.848, Test: 0.780\n",
            "Epoch: 078 | LOSS: Train: 3.5424 Val: 4.0455 Test: 4.5503  | ROC: Train: 0.893, Val: 0.846, Test: 0.779\n",
            "Epoch: 079 | LOSS: Train: 3.5330 Val: 3.9593 Test: 4.4916  | ROC: Train: 0.890, Val: 0.849, Test: 0.777\n",
            "Epoch: 080 | LOSS: Train: 3.5261 Val: 3.9953 Test: 4.4559  | ROC: Train: 0.892, Val: 0.849, Test: 0.791\n",
            "Epoch: 081 | LOSS: Train: 3.6266 Val: 3.9136 Test: 4.4401  | ROC: Train: 0.885, Val: 0.847, Test: 0.779\n",
            "Epoch: 082 | LOSS: Train: 3.5372 Val: 4.0167 Test: 4.4985  | ROC: Train: 0.889, Val: 0.839, Test: 0.781\n",
            "Epoch: 083 | LOSS: Train: 3.5187 Val: 3.9117 Test: 4.3971  | ROC: Train: 0.893, Val: 0.840, Test: 0.782\n",
            "Epoch: 084 | LOSS: Train: 3.5067 Val: 4.1302 Test: 4.6143  | ROC: Train: 0.896, Val: 0.838, Test: 0.786\n",
            "Epoch: 085 | LOSS: Train: 3.4848 Val: 4.0396 Test: 4.5266  | ROC: Train: 0.895, Val: 0.840, Test: 0.786\n",
            "Epoch: 086 | LOSS: Train: 3.5003 Val: 3.9450 Test: 4.3674  | ROC: Train: 0.894, Val: 0.843, Test: 0.786\n",
            "Epoch: 087 | LOSS: Train: 3.4770 Val: 3.9989 Test: 4.4873  | ROC: Train: 0.897, Val: 0.840, Test: 0.783\n",
            "Epoch: 088 | LOSS: Train: 3.4893 Val: 3.9928 Test: 4.4775  | ROC: Train: 0.895, Val: 0.828, Test: 0.776\n",
            "Epoch: 089 | LOSS: Train: 3.4912 Val: 3.8732 Test: 4.3555  | ROC: Train: 0.897, Val: 0.845, Test: 0.785\n",
            "Epoch: 090 | LOSS: Train: 3.4455 Val: 3.9579 Test: 4.4351  | ROC: Train: 0.896, Val: 0.850, Test: 0.788\n",
            "Epoch: 091 | LOSS: Train: 3.4262 Val: 3.9075 Test: 4.3834  | ROC: Train: 0.901, Val: 0.846, Test: 0.785\n",
            "Epoch: 092 | LOSS: Train: 3.3961 Val: 3.9350 Test: 4.4555  | ROC: Train: 0.900, Val: 0.850, Test: 0.784\n",
            "Epoch: 093 | LOSS: Train: 3.4415 Val: 4.1844 Test: 4.7121  | ROC: Train: 0.901, Val: 0.845, Test: 0.781\n",
            "Epoch: 094 | LOSS: Train: 3.4513 Val: 3.9574 Test: 4.4530  | ROC: Train: 0.896, Val: 0.846, Test: 0.786\n",
            "Epoch: 095 | LOSS: Train: 3.4395 Val: 3.8665 Test: 4.3624  | ROC: Train: 0.901, Val: 0.847, Test: 0.788\n",
            "Epoch: 096 | LOSS: Train: 3.4946 Val: 3.8529 Test: 4.3399  | ROC: Train: 0.897, Val: 0.848, Test: 0.785\n",
            "Epoch: 097 | LOSS: Train: 3.3877 Val: 3.9217 Test: 4.4104  | ROC: Train: 0.903, Val: 0.849, Test: 0.785\n",
            "Epoch: 098 | LOSS: Train: 3.3646 Val: 3.8644 Test: 4.3706  | ROC: Train: 0.903, Val: 0.854, Test: 0.787\n",
            "Epoch: 099 | LOSS: Train: 3.3552 Val: 3.9863 Test: 4.5140  | ROC: Train: 0.907, Val: 0.840, Test: 0.783\n",
            "Epoch: 100 | LOSS: Train: 3.3719 Val: 4.2842 Test: 4.7411  | ROC: Train: 0.906, Val: 0.859, Test: 0.793\n",
            "Epoch: 101 | LOSS: Train: 3.4450 Val: 4.5585 Test: 5.0469  | ROC: Train: 0.900, Val: 0.861, Test: 0.790\n",
            "Epoch: 102 | LOSS: Train: 3.4225 Val: 3.9375 Test: 4.4478  | ROC: Train: 0.902, Val: 0.856, Test: 0.789\n",
            "Epoch: 103 | LOSS: Train: 3.3547 Val: 4.0134 Test: 4.4986  | ROC: Train: 0.905, Val: 0.848, Test: 0.785\n",
            "Epoch: 104 | LOSS: Train: 3.3321 Val: 3.8491 Test: 4.3333  | ROC: Train: 0.904, Val: 0.844, Test: 0.787\n",
            "Epoch: 105 | LOSS: Train: 3.3108 Val: 3.9194 Test: 4.4464  | ROC: Train: 0.909, Val: 0.850, Test: 0.788\n",
            "Epoch: 106 | LOSS: Train: 3.3145 Val: 3.8605 Test: 4.3512  | ROC: Train: 0.909, Val: 0.847, Test: 0.788\n",
            "Epoch: 107 | LOSS: Train: 3.3202 Val: 3.8198 Test: 4.3294  | ROC: Train: 0.908, Val: 0.854, Test: 0.789\n",
            "Epoch: 108 | LOSS: Train: 3.2854 Val: 4.2545 Test: 4.7587  | ROC: Train: 0.909, Val: 0.852, Test: 0.790\n",
            "Epoch: 109 | LOSS: Train: 3.2963 Val: 3.8826 Test: 4.4431  | ROC: Train: 0.912, Val: 0.856, Test: 0.786\n",
            "Epoch: 110 | LOSS: Train: 3.3022 Val: 3.9295 Test: 4.4840  | ROC: Train: 0.910, Val: 0.850, Test: 0.785\n",
            "Epoch: 111 | LOSS: Train: 3.3428 Val: 3.8322 Test: 4.3805  | ROC: Train: 0.906, Val: 0.851, Test: 0.786\n",
            "Epoch: 112 | LOSS: Train: 3.3070 Val: 3.8271 Test: 4.3430  | ROC: Train: 0.907, Val: 0.855, Test: 0.790\n",
            "Epoch: 113 | LOSS: Train: 3.2580 Val: 3.9468 Test: 4.5321  | ROC: Train: 0.911, Val: 0.841, Test: 0.780\n",
            "Epoch: 114 | LOSS: Train: 3.4157 Val: 4.3108 Test: 4.7906  | ROC: Train: 0.902, Val: 0.863, Test: 0.792\n",
            "Epoch: 115 | LOSS: Train: 3.3038 Val: 3.8029 Test: 4.3499  | ROC: Train: 0.909, Val: 0.854, Test: 0.788\n",
            "Epoch: 116 | LOSS: Train: 3.2232 Val: 3.9652 Test: 4.4407  | ROC: Train: 0.915, Val: 0.852, Test: 0.794\n",
            "Epoch: 117 | LOSS: Train: 3.2187 Val: 3.8486 Test: 4.3761  | ROC: Train: 0.915, Val: 0.863, Test: 0.796\n",
            "Epoch: 118 | LOSS: Train: 3.2005 Val: 3.8925 Test: 4.4576  | ROC: Train: 0.913, Val: 0.853, Test: 0.791\n",
            "Epoch: 119 | LOSS: Train: 3.1960 Val: 4.1368 Test: 4.7360  | ROC: Train: 0.914, Val: 0.842, Test: 0.782\n",
            "Epoch: 120 | LOSS: Train: 3.2691 Val: 3.8956 Test: 4.4111  | ROC: Train: 0.915, Val: 0.847, Test: 0.794\n",
            "Epoch: 121 | LOSS: Train: 3.1903 Val: 3.8657 Test: 4.4443  | ROC: Train: 0.917, Val: 0.855, Test: 0.791\n",
            "Epoch: 122 | LOSS: Train: 3.1617 Val: 3.8270 Test: 4.3917  | ROC: Train: 0.920, Val: 0.842, Test: 0.783\n",
            "Epoch: 123 | LOSS: Train: 3.1483 Val: 3.7890 Test: 4.3217  | ROC: Train: 0.918, Val: 0.857, Test: 0.795\n",
            "Epoch: 124 | LOSS: Train: 3.1685 Val: 3.8293 Test: 4.4196  | ROC: Train: 0.920, Val: 0.851, Test: 0.788\n",
            "Epoch: 125 | LOSS: Train: 3.1473 Val: 3.8331 Test: 4.3372  | ROC: Train: 0.919, Val: 0.851, Test: 0.797\n",
            "Epoch: 126 | LOSS: Train: 3.1869 Val: 3.9868 Test: 4.5648  | ROC: Train: 0.915, Val: 0.849, Test: 0.784\n",
            "Epoch: 127 | LOSS: Train: 3.1211 Val: 3.8421 Test: 4.4007  | ROC: Train: 0.919, Val: 0.857, Test: 0.792\n",
            "Epoch: 128 | LOSS: Train: 3.1413 Val: 3.8471 Test: 4.4051  | ROC: Train: 0.920, Val: 0.859, Test: 0.794\n",
            "Epoch: 129 | LOSS: Train: 3.1893 Val: 3.8724 Test: 4.4143  | ROC: Train: 0.918, Val: 0.846, Test: 0.795\n",
            "Epoch: 130 | LOSS: Train: 3.0619 Val: 3.7517 Test: 4.3508  | ROC: Train: 0.925, Val: 0.856, Test: 0.789\n",
            "Epoch: 131 | LOSS: Train: 3.0572 Val: 4.0382 Test: 4.6161  | ROC: Train: 0.925, Val: 0.846, Test: 0.791\n",
            "Epoch: 132 | LOSS: Train: 3.1476 Val: 3.8467 Test: 4.5047  | ROC: Train: 0.921, Val: 0.856, Test: 0.788\n",
            "Epoch: 133 | LOSS: Train: 3.1251 Val: 3.8548 Test: 4.3972  | ROC: Train: 0.923, Val: 0.852, Test: 0.793\n",
            "Epoch: 134 | LOSS: Train: 3.0478 Val: 4.1050 Test: 4.7182  | ROC: Train: 0.927, Val: 0.846, Test: 0.791\n",
            "Epoch: 135 | LOSS: Train: 3.0782 Val: 3.8557 Test: 4.4000  | ROC: Train: 0.926, Val: 0.858, Test: 0.797\n",
            "Epoch: 136 | LOSS: Train: 3.0395 Val: 3.8284 Test: 4.4036  | ROC: Train: 0.928, Val: 0.849, Test: 0.792\n",
            "Epoch: 137 | LOSS: Train: 3.0043 Val: 3.8724 Test: 4.4872  | ROC: Train: 0.930, Val: 0.854, Test: 0.793\n",
            "Epoch: 138 | LOSS: Train: 3.0594 Val: 3.8746 Test: 4.4212  | ROC: Train: 0.929, Val: 0.863, Test: 0.802\n",
            "Epoch: 139 | LOSS: Train: 3.0065 Val: 3.8668 Test: 4.3798  | ROC: Train: 0.931, Val: 0.856, Test: 0.802\n",
            "Epoch: 140 | LOSS: Train: 3.0319 Val: 4.0736 Test: 4.6152  | ROC: Train: 0.929, Val: 0.858, Test: 0.801\n",
            "Epoch: 141 | LOSS: Train: 3.1239 Val: 3.8535 Test: 4.3671  | ROC: Train: 0.924, Val: 0.854, Test: 0.799\n",
            "Epoch: 142 | LOSS: Train: 3.0127 Val: 3.9407 Test: 4.4907  | ROC: Train: 0.928, Val: 0.850, Test: 0.794\n",
            "Epoch: 143 | LOSS: Train: 2.9218 Val: 4.0565 Test: 4.6301  | ROC: Train: 0.936, Val: 0.850, Test: 0.793\n",
            "Epoch: 144 | LOSS: Train: 3.0047 Val: 3.8646 Test: 4.5138  | ROC: Train: 0.927, Val: 0.853, Test: 0.789\n",
            "Epoch: 145 | LOSS: Train: 3.0795 Val: 3.8482 Test: 4.4847  | ROC: Train: 0.928, Val: 0.836, Test: 0.779\n",
            "Epoch: 146 | LOSS: Train: 3.0253 Val: 3.9912 Test: 4.6014  | ROC: Train: 0.929, Val: 0.843, Test: 0.792\n",
            "Epoch: 147 | LOSS: Train: 2.9074 Val: 3.9091 Test: 4.5308  | ROC: Train: 0.938, Val: 0.848, Test: 0.790\n",
            "Epoch: 148 | LOSS: Train: 2.9246 Val: 3.8637 Test: 4.5289  | ROC: Train: 0.933, Val: 0.857, Test: 0.791\n",
            "Epoch: 149 | LOSS: Train: 2.9118 Val: 4.1293 Test: 4.7616  | ROC: Train: 0.936, Val: 0.864, Test: 0.797\n",
            "Epoch: 150 | LOSS: Train: 2.9360 Val: 3.8238 Test: 4.4390  | ROC: Train: 0.936, Val: 0.854, Test: 0.794\n",
            "Epoch: 151 | LOSS: Train: 2.9804 Val: 3.8356 Test: 4.4796  | ROC: Train: 0.933, Val: 0.852, Test: 0.786\n",
            "Epoch: 152 | LOSS: Train: 2.9216 Val: 3.8771 Test: 4.4467  | ROC: Train: 0.935, Val: 0.854, Test: 0.801\n",
            "Epoch: 153 | LOSS: Train: 2.8350 Val: 4.1096 Test: 4.8002  | ROC: Train: 0.940, Val: 0.845, Test: 0.786\n",
            "Epoch: 154 | LOSS: Train: 2.9136 Val: 3.7988 Test: 4.4656  | ROC: Train: 0.935, Val: 0.862, Test: 0.796\n",
            "Epoch: 155 | LOSS: Train: 2.8494 Val: 3.8522 Test: 4.5340  | ROC: Train: 0.942, Val: 0.849, Test: 0.788\n",
            "Epoch: 156 | LOSS: Train: 2.8618 Val: 3.8573 Test: 4.4648  | ROC: Train: 0.940, Val: 0.856, Test: 0.798\n",
            "Epoch: 157 | LOSS: Train: 2.8599 Val: 3.9512 Test: 4.6869  | ROC: Train: 0.938, Val: 0.841, Test: 0.782\n",
            "Epoch: 158 | LOSS: Train: 2.8222 Val: 3.9185 Test: 4.5763  | ROC: Train: 0.943, Val: 0.852, Test: 0.795\n",
            "Epoch: 159 | LOSS: Train: 2.9043 Val: 4.3477 Test: 5.0517  | ROC: Train: 0.937, Val: 0.839, Test: 0.791\n",
            "Epoch: 160 | LOSS: Train: 2.9533 Val: 3.8731 Test: 4.5734  | ROC: Train: 0.937, Val: 0.857, Test: 0.788\n",
            "Epoch: 161 | LOSS: Train: 2.8118 Val: 3.9502 Test: 4.6219  | ROC: Train: 0.941, Val: 0.844, Test: 0.792\n",
            "Epoch: 162 | LOSS: Train: 2.8803 Val: 3.7898 Test: 4.5212  | ROC: Train: 0.937, Val: 0.853, Test: 0.789\n",
            "Epoch: 163 | LOSS: Train: 2.8696 Val: 3.9066 Test: 4.6194  | ROC: Train: 0.938, Val: 0.847, Test: 0.788\n",
            "Epoch: 164 | LOSS: Train: 2.7899 Val: 3.9287 Test: 4.5466  | ROC: Train: 0.946, Val: 0.846, Test: 0.795\n",
            "Epoch: 165 | LOSS: Train: 2.7697 Val: 3.9205 Test: 4.5096  | ROC: Train: 0.946, Val: 0.845, Test: 0.797\n",
            "Epoch: 166 | LOSS: Train: 2.7816 Val: 3.9339 Test: 4.6530  | ROC: Train: 0.943, Val: 0.847, Test: 0.793\n",
            "Epoch: 167 | LOSS: Train: 2.8576 Val: 3.8164 Test: 4.5437  | ROC: Train: 0.938, Val: 0.843, Test: 0.783\n",
            "Epoch: 168 | LOSS: Train: 2.8823 Val: 3.9157 Test: 4.6234  | ROC: Train: 0.942, Val: 0.843, Test: 0.785\n",
            "Epoch: 169 | LOSS: Train: 2.7960 Val: 3.9870 Test: 4.6454  | ROC: Train: 0.944, Val: 0.854, Test: 0.795\n",
            "Epoch: 170 | LOSS: Train: 2.6937 Val: 3.9216 Test: 4.6523  | ROC: Train: 0.951, Val: 0.847, Test: 0.790\n",
            "Epoch: 171 | LOSS: Train: 2.7307 Val: 3.8265 Test: 4.5364  | ROC: Train: 0.947, Val: 0.850, Test: 0.792\n",
            "Epoch: 172 | LOSS: Train: 2.6946 Val: 3.8698 Test: 4.5838  | ROC: Train: 0.950, Val: 0.852, Test: 0.788\n",
            "Epoch: 173 | LOSS: Train: 2.7469 Val: 3.8324 Test: 4.5982  | ROC: Train: 0.946, Val: 0.853, Test: 0.789\n",
            "Epoch: 174 | LOSS: Train: 2.8079 Val: 4.1405 Test: 4.9177  | ROC: Train: 0.943, Val: 0.839, Test: 0.786\n",
            "Epoch: 175 | LOSS: Train: 2.7178 Val: 3.9624 Test: 4.7012  | ROC: Train: 0.948, Val: 0.856, Test: 0.788\n",
            "Epoch: 176 | LOSS: Train: 2.6972 Val: 3.8626 Test: 4.5285  | ROC: Train: 0.949, Val: 0.852, Test: 0.794\n",
            "Epoch: 177 | LOSS: Train: 2.6461 Val: 3.8543 Test: 4.6711  | ROC: Train: 0.950, Val: 0.845, Test: 0.783\n",
            "Epoch: 178 | LOSS: Train: 2.7922 Val: 4.3060 Test: 5.0656  | ROC: Train: 0.943, Val: 0.837, Test: 0.787\n",
            "Epoch: 179 | LOSS: Train: 2.7342 Val: 3.8927 Test: 4.6044  | ROC: Train: 0.948, Val: 0.849, Test: 0.792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "\n",
        "PATH = f\"/content/drive/MyDrive/AI/covid-1/covid-GraphConv-3x--200-d02--1.pt\"\n",
        "torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': train_loss,\n",
        "        }, PATH)\n",
        "torch.save(model,  PATH.strip(\".pt\") + \"-model1.pt\" )"
      ],
      "metadata": {
        "id": "Kx7pRXmseOIP"
      },
      "id": "Kx7pRXmseOIP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the 'Conv-GRU' Network"
      ],
      "metadata": {
        "id": "fruoyzlnWvgK"
      },
      "id": "fruoyzlnWvgK"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install class-resolver\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GraphConv, GraphSAGE\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "from torch.nn import GRUCell\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(Net, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GraphConv(44, hidden_channels)\n",
        "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.gru1 = GRUCell(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.gru2 = GRUCell(hidden_channels, hidden_channels)\n",
        "        self.gru3 = GRUCell(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.lin = Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.gru1(x, x)\n",
        "        x = x.relu()\n",
        "        x = self.gru2(x, x)\n",
        "        x = x.relu()\n",
        "        x = self.gru3(x, x)\n",
        "\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_add_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        # x = x.sigmoid()\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = Net(hidden_channels=200)\n",
        "print(model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "WEIGHT = 230\n",
        "print_every = 0\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=10**-3.5,\n",
        "                             weight_decay=10**-2.9)\n",
        "\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    train_loss, train_roc = train()\n",
        "    val_loss, val_roc = test(val_loader)\n",
        "    test_loss, test_roc = test(test_loader)\n",
        "    # train_loss, train_roc = test(train_loader)\n",
        "    print(f'Epoch: {epoch:03d} | LOSS: Train: {train_loss:.4f} Val: {val_loss:.4f} Test: {test_loss:.4f}  | ROC: Train: {train_roc:.3f}, Val: {val_roc:.3f}, '\n",
        "          f'Test: {test_roc:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG-GHgRgeM7P",
        "outputId": "315f9b84-5f73-4101-b973-9c8df421f7bd"
      },
      "id": "AG-GHgRgeM7P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): GraphConv(44, 200)\n",
            "  (conv2): GraphConv(200, 200)\n",
            "  (gru1): GRUCell(200, 200)\n",
            "  (gru2): GRUCell(200, 200)\n",
            "  (gru3): GRUCell(200, 200)\n",
            "  (conv3): GraphConv(200, 200)\n",
            "  (lin): Linear(in_features=200, out_features=1, bias=True)\n",
            ")\n",
            "Epoch: 001 | LOSS: Train: 19.0634 Val: 5.5373 Test: 5.5719  | ROC: Train: 0.491, Val: 0.440, Test: 0.453\n",
            "Epoch: 002 | LOSS: Train: 5.5375 Val: 5.4603 Test: 5.4997  | ROC: Train: 0.492, Val: 0.456, Test: 0.473\n",
            "Epoch: 003 | LOSS: Train: 5.4619 Val: 5.3818 Test: 5.4223  | ROC: Train: 0.534, Val: 0.506, Test: 0.526\n",
            "Epoch: 004 | LOSS: Train: 5.3507 Val: 5.3946 Test: 5.4467  | ROC: Train: 0.601, Val: 0.607, Test: 0.607\n",
            "Epoch: 005 | LOSS: Train: 5.2396 Val: 5.1992 Test: 5.2697  | ROC: Train: 0.671, Val: 0.669, Test: 0.651\n",
            "Epoch: 006 | LOSS: Train: 5.1737 Val: 5.1930 Test: 5.2733  | ROC: Train: 0.701, Val: 0.683, Test: 0.667\n",
            "Epoch: 007 | LOSS: Train: 5.1160 Val: 5.1030 Test: 5.1886  | ROC: Train: 0.711, Val: 0.695, Test: 0.684\n",
            "Epoch: 008 | LOSS: Train: 5.0403 Val: 5.0736 Test: 5.1830  | ROC: Train: 0.729, Val: 0.709, Test: 0.698\n",
            "Epoch: 009 | LOSS: Train: 4.9431 Val: 4.9736 Test: 5.0968  | ROC: Train: 0.748, Val: 0.720, Test: 0.713\n",
            "Epoch: 010 | LOSS: Train: 4.8928 Val: 5.0850 Test: 5.2215  | ROC: Train: 0.757, Val: 0.729, Test: 0.726\n",
            "Epoch: 011 | LOSS: Train: 4.8399 Val: 5.0994 Test: 5.2478  | ROC: Train: 0.764, Val: 0.742, Test: 0.736\n",
            "Epoch: 012 | LOSS: Train: 4.8144 Val: 4.8987 Test: 5.0702  | ROC: Train: 0.768, Val: 0.738, Test: 0.738\n",
            "Epoch: 013 | LOSS: Train: 4.7069 Val: 4.7999 Test: 5.0239  | ROC: Train: 0.784, Val: 0.738, Test: 0.734\n",
            "Epoch: 014 | LOSS: Train: 4.6601 Val: 4.7674 Test: 4.9758  | ROC: Train: 0.793, Val: 0.755, Test: 0.749\n",
            "Epoch: 015 | LOSS: Train: 4.6690 Val: 4.9689 Test: 5.1640  | ROC: Train: 0.783, Val: 0.774, Test: 0.759\n",
            "Epoch: 016 | LOSS: Train: 4.6033 Val: 4.8821 Test: 5.1371  | ROC: Train: 0.799, Val: 0.773, Test: 0.760\n",
            "Epoch: 017 | LOSS: Train: 4.5426 Val: 4.5894 Test: 4.8277  | ROC: Train: 0.802, Val: 0.767, Test: 0.767\n",
            "Epoch: 018 | LOSS: Train: 4.5230 Val: 4.5965 Test: 4.8652  | ROC: Train: 0.798, Val: 0.784, Test: 0.767\n",
            "Epoch: 019 | LOSS: Train: 4.5502 Val: 4.6304 Test: 4.8980  | ROC: Train: 0.797, Val: 0.788, Test: 0.766\n",
            "Epoch: 020 | LOSS: Train: 4.4018 Val: 4.5321 Test: 4.7839  | ROC: Train: 0.819, Val: 0.791, Test: 0.773\n",
            "Epoch: 021 | LOSS: Train: 4.4106 Val: 4.4689 Test: 4.7457  | ROC: Train: 0.810, Val: 0.786, Test: 0.771\n",
            "Epoch: 022 | LOSS: Train: 4.2912 Val: 4.5118 Test: 4.7629  | ROC: Train: 0.823, Val: 0.803, Test: 0.779\n",
            "Epoch: 023 | LOSS: Train: 4.2980 Val: 4.5234 Test: 4.7980  | ROC: Train: 0.815, Val: 0.788, Test: 0.777\n",
            "Epoch: 024 | LOSS: Train: 4.2377 Val: 4.3632 Test: 4.6242  | ROC: Train: 0.820, Val: 0.791, Test: 0.778\n",
            "Epoch: 025 | LOSS: Train: 4.1362 Val: 4.6764 Test: 4.9263  | ROC: Train: 0.832, Val: 0.782, Test: 0.775\n",
            "Epoch: 026 | LOSS: Train: 4.1984 Val: 5.5087 Test: 5.7908  | ROC: Train: 0.822, Val: 0.811, Test: 0.781\n",
            "Epoch: 027 | LOSS: Train: 4.3402 Val: 4.2917 Test: 4.5610  | ROC: Train: 0.803, Val: 0.806, Test: 0.780\n",
            "Epoch: 028 | LOSS: Train: 4.0455 Val: 4.2940 Test: 4.5475  | ROC: Train: 0.834, Val: 0.809, Test: 0.784\n",
            "Epoch: 029 | LOSS: Train: 4.0301 Val: 4.3799 Test: 4.6992  | ROC: Train: 0.836, Val: 0.812, Test: 0.782\n",
            "Epoch: 030 | LOSS: Train: 4.0693 Val: 4.2479 Test: 4.5621  | ROC: Train: 0.833, Val: 0.817, Test: 0.782\n",
            "Epoch: 031 | LOSS: Train: 3.9904 Val: 4.2228 Test: 4.5040  | ROC: Train: 0.840, Val: 0.814, Test: 0.787\n",
            "Epoch: 032 | LOSS: Train: 4.0464 Val: 4.4268 Test: 4.7090  | ROC: Train: 0.835, Val: 0.804, Test: 0.781\n",
            "Epoch: 033 | LOSS: Train: 3.9941 Val: 4.1770 Test: 4.4867  | ROC: Train: 0.834, Val: 0.811, Test: 0.779\n",
            "Epoch: 034 | LOSS: Train: 3.9292 Val: 4.1810 Test: 4.4501  | ROC: Train: 0.841, Val: 0.817, Test: 0.787\n",
            "Epoch: 035 | LOSS: Train: 3.9262 Val: 4.2420 Test: 4.6132  | ROC: Train: 0.839, Val: 0.831, Test: 0.784\n",
            "Epoch: 036 | LOSS: Train: 3.8913 Val: 4.2862 Test: 4.5812  | ROC: Train: 0.845, Val: 0.817, Test: 0.785\n",
            "Epoch: 037 | LOSS: Train: 3.8684 Val: 4.0739 Test: 4.3622  | ROC: Train: 0.842, Val: 0.810, Test: 0.786\n",
            "Epoch: 038 | LOSS: Train: 3.8686 Val: 4.0676 Test: 4.4345  | ROC: Train: 0.844, Val: 0.834, Test: 0.794\n",
            "Epoch: 039 | LOSS: Train: 3.7849 Val: 4.0749 Test: 4.3428  | ROC: Train: 0.853, Val: 0.819, Test: 0.797\n",
            "Epoch: 040 | LOSS: Train: 3.7642 Val: 4.2777 Test: 4.6645  | ROC: Train: 0.851, Val: 0.835, Test: 0.792\n",
            "Epoch: 041 | LOSS: Train: 3.8673 Val: 4.0321 Test: 4.4064  | ROC: Train: 0.845, Val: 0.832, Test: 0.792\n",
            "Epoch: 042 | LOSS: Train: 3.8262 Val: 3.9967 Test: 4.3902  | ROC: Train: 0.842, Val: 0.832, Test: 0.795\n",
            "Epoch: 043 | LOSS: Train: 3.7136 Val: 3.9998 Test: 4.3688  | ROC: Train: 0.854, Val: 0.827, Test: 0.796\n",
            "Epoch: 044 | LOSS: Train: 3.7497 Val: 3.9763 Test: 4.3332  | ROC: Train: 0.854, Val: 0.823, Test: 0.796\n",
            "Epoch: 045 | LOSS: Train: 3.6794 Val: 3.9970 Test: 4.3597  | ROC: Train: 0.854, Val: 0.831, Test: 0.799\n",
            "Epoch: 046 | LOSS: Train: 3.6884 Val: 4.1813 Test: 4.6310  | ROC: Train: 0.859, Val: 0.841, Test: 0.793\n",
            "Epoch: 047 | LOSS: Train: 3.6744 Val: 3.9670 Test: 4.3798  | ROC: Train: 0.854, Val: 0.835, Test: 0.796\n",
            "Epoch: 048 | LOSS: Train: 3.7793 Val: 3.9784 Test: 4.3837  | ROC: Train: 0.847, Val: 0.828, Test: 0.795\n",
            "Epoch: 049 | LOSS: Train: 3.6518 Val: 3.9767 Test: 4.3338  | ROC: Train: 0.859, Val: 0.833, Test: 0.807\n",
            "Epoch: 050 | LOSS: Train: 3.6853 Val: 4.0389 Test: 4.4118  | ROC: Train: 0.851, Val: 0.830, Test: 0.805\n",
            "Epoch: 051 | LOSS: Train: 3.6272 Val: 3.9636 Test: 4.3906  | ROC: Train: 0.859, Val: 0.835, Test: 0.805\n",
            "Epoch: 052 | LOSS: Train: 3.6612 Val: 3.8787 Test: 4.2424  | ROC: Train: 0.857, Val: 0.826, Test: 0.801\n",
            "Epoch: 053 | LOSS: Train: 3.6078 Val: 3.8791 Test: 4.3162  | ROC: Train: 0.858, Val: 0.833, Test: 0.804\n",
            "Epoch: 054 | LOSS: Train: 3.5982 Val: 3.9844 Test: 4.4369  | ROC: Train: 0.859, Val: 0.847, Test: 0.807\n",
            "Epoch: 055 | LOSS: Train: 3.6185 Val: 3.8294 Test: 4.1980  | ROC: Train: 0.857, Val: 0.830, Test: 0.804\n",
            "Epoch: 056 | LOSS: Train: 3.5487 Val: 3.8671 Test: 4.3233  | ROC: Train: 0.864, Val: 0.848, Test: 0.810\n",
            "Epoch: 057 | LOSS: Train: 3.5759 Val: 3.8421 Test: 4.2491  | ROC: Train: 0.864, Val: 0.832, Test: 0.801\n",
            "Epoch: 058 | LOSS: Train: 3.5243 Val: 3.8915 Test: 4.2600  | ROC: Train: 0.867, Val: 0.838, Test: 0.805\n",
            "Epoch: 059 | LOSS: Train: 3.5069 Val: 4.0121 Test: 4.4693  | ROC: Train: 0.865, Val: 0.840, Test: 0.798\n",
            "Epoch: 060 | LOSS: Train: 3.5091 Val: 3.8214 Test: 4.2352  | ROC: Train: 0.864, Val: 0.835, Test: 0.811\n",
            "Epoch: 061 | LOSS: Train: 3.5141 Val: 3.9075 Test: 4.3205  | ROC: Train: 0.867, Val: 0.838, Test: 0.805\n",
            "Epoch: 062 | LOSS: Train: 3.4974 Val: 4.0931 Test: 4.5751  | ROC: Train: 0.866, Val: 0.847, Test: 0.806\n",
            "Epoch: 063 | LOSS: Train: 3.6055 Val: 4.0200 Test: 4.4057  | ROC: Train: 0.857, Val: 0.844, Test: 0.809\n",
            "Epoch: 064 | LOSS: Train: 3.5444 Val: 3.8234 Test: 4.2546  | ROC: Train: 0.865, Val: 0.842, Test: 0.806\n",
            "Epoch: 065 | LOSS: Train: 3.5007 Val: 3.8781 Test: 4.3088  | ROC: Train: 0.862, Val: 0.845, Test: 0.808\n",
            "Epoch: 066 | LOSS: Train: 3.4609 Val: 3.8572 Test: 4.2579  | ROC: Train: 0.868, Val: 0.838, Test: 0.802\n",
            "Epoch: 067 | LOSS: Train: 3.4465 Val: 4.0128 Test: 4.5004  | ROC: Train: 0.875, Val: 0.846, Test: 0.803\n",
            "Epoch: 068 | LOSS: Train: 3.6073 Val: 3.9489 Test: 4.4174  | ROC: Train: 0.859, Val: 0.838, Test: 0.807\n",
            "Epoch: 069 | LOSS: Train: 3.5290 Val: 3.9054 Test: 4.2962  | ROC: Train: 0.866, Val: 0.844, Test: 0.810\n",
            "Epoch: 070 | LOSS: Train: 3.4076 Val: 3.7830 Test: 4.2019  | ROC: Train: 0.872, Val: 0.840, Test: 0.813\n",
            "Epoch: 071 | LOSS: Train: 3.4199 Val: 3.9596 Test: 4.3768  | ROC: Train: 0.867, Val: 0.846, Test: 0.815\n",
            "Epoch: 072 | LOSS: Train: 3.5154 Val: 3.9175 Test: 4.3297  | ROC: Train: 0.868, Val: 0.848, Test: 0.816\n",
            "Epoch: 073 | LOSS: Train: 3.4152 Val: 3.8579 Test: 4.3073  | ROC: Train: 0.873, Val: 0.842, Test: 0.812\n",
            "Epoch: 074 | LOSS: Train: 3.4024 Val: 3.8106 Test: 4.2202  | ROC: Train: 0.874, Val: 0.844, Test: 0.814\n",
            "Epoch: 075 | LOSS: Train: 3.4057 Val: 3.7972 Test: 4.1924  | ROC: Train: 0.871, Val: 0.838, Test: 0.816\n",
            "Epoch: 076 | LOSS: Train: 3.3476 Val: 3.8487 Test: 4.2068  | ROC: Train: 0.878, Val: 0.841, Test: 0.817\n",
            "Epoch: 077 | LOSS: Train: 3.4101 Val: 3.9297 Test: 4.2552  | ROC: Train: 0.869, Val: 0.827, Test: 0.814\n",
            "Epoch: 078 | LOSS: Train: 3.4486 Val: 3.9160 Test: 4.2500  | ROC: Train: 0.866, Val: 0.832, Test: 0.815\n",
            "Epoch: 079 | LOSS: Train: 3.3442 Val: 3.8004 Test: 4.2092  | ROC: Train: 0.881, Val: 0.841, Test: 0.812\n",
            "Epoch: 080 | LOSS: Train: 3.3097 Val: 4.0009 Test: 4.3852  | ROC: Train: 0.880, Val: 0.848, Test: 0.815\n",
            "Epoch: 081 | LOSS: Train: 3.3649 Val: 3.8500 Test: 4.2458  | ROC: Train: 0.878, Val: 0.840, Test: 0.818\n",
            "Epoch: 082 | LOSS: Train: 3.3118 Val: 3.7594 Test: 4.1610  | ROC: Train: 0.878, Val: 0.849, Test: 0.822\n",
            "Epoch: 083 | LOSS: Train: 3.2666 Val: 3.8734 Test: 4.2685  | ROC: Train: 0.883, Val: 0.843, Test: 0.816\n",
            "Epoch: 084 | LOSS: Train: 3.3194 Val: 3.9889 Test: 4.3801  | ROC: Train: 0.876, Val: 0.824, Test: 0.807\n",
            "Epoch: 085 | LOSS: Train: 3.2907 Val: 3.9068 Test: 4.3657  | ROC: Train: 0.875, Val: 0.843, Test: 0.807\n",
            "Epoch: 086 | LOSS: Train: 3.3189 Val: 3.8332 Test: 4.1958  | ROC: Train: 0.876, Val: 0.837, Test: 0.815\n",
            "Epoch: 087 | LOSS: Train: 3.3728 Val: 3.8462 Test: 4.2410  | ROC: Train: 0.880, Val: 0.823, Test: 0.798\n",
            "Epoch: 088 | LOSS: Train: 3.2476 Val: 3.8144 Test: 4.2471  | ROC: Train: 0.884, Val: 0.848, Test: 0.818\n",
            "Epoch: 089 | LOSS: Train: 3.2668 Val: 3.8765 Test: 4.2805  | ROC: Train: 0.882, Val: 0.838, Test: 0.812\n",
            "Epoch: 090 | LOSS: Train: 3.2969 Val: 4.2056 Test: 4.6651  | ROC: Train: 0.883, Val: 0.848, Test: 0.818\n",
            "Epoch: 091 | LOSS: Train: 3.2928 Val: 3.8476 Test: 4.2579  | ROC: Train: 0.878, Val: 0.843, Test: 0.807\n",
            "Epoch: 092 | LOSS: Train: 3.2472 Val: 3.8044 Test: 4.1717  | ROC: Train: 0.884, Val: 0.832, Test: 0.815\n",
            "Epoch: 093 | LOSS: Train: 3.2048 Val: 3.8068 Test: 4.1507  | ROC: Train: 0.887, Val: 0.841, Test: 0.822\n",
            "Epoch: 094 | LOSS: Train: 3.2575 Val: 3.8516 Test: 4.2662  | ROC: Train: 0.882, Val: 0.832, Test: 0.811\n",
            "Epoch: 095 | LOSS: Train: 3.2225 Val: 3.8235 Test: 4.2066  | ROC: Train: 0.885, Val: 0.834, Test: 0.813\n",
            "Epoch: 096 | LOSS: Train: 3.1818 Val: 3.9239 Test: 4.3095  | ROC: Train: 0.891, Val: 0.829, Test: 0.814\n",
            "Epoch: 097 | LOSS: Train: 3.2454 Val: 3.8602 Test: 4.2609  | ROC: Train: 0.885, Val: 0.833, Test: 0.818\n",
            "Epoch: 098 | LOSS: Train: 3.2243 Val: 3.8907 Test: 4.3670  | ROC: Train: 0.888, Val: 0.836, Test: 0.817\n",
            "Epoch: 099 | LOSS: Train: 3.1902 Val: 3.7798 Test: 4.1424  | ROC: Train: 0.892, Val: 0.835, Test: 0.820\n",
            "Epoch: 100 | LOSS: Train: 3.2510 Val: 3.8671 Test: 4.2400  | ROC: Train: 0.884, Val: 0.820, Test: 0.801\n",
            "Epoch: 101 | LOSS: Train: 3.1854 Val: 3.9177 Test: 4.3872  | ROC: Train: 0.889, Val: 0.846, Test: 0.820\n",
            "Epoch: 102 | LOSS: Train: 3.2053 Val: 3.8338 Test: 4.2014  | ROC: Train: 0.891, Val: 0.831, Test: 0.819\n",
            "Epoch: 103 | LOSS: Train: 3.1492 Val: 4.0368 Test: 4.3838  | ROC: Train: 0.894, Val: 0.837, Test: 0.817\n",
            "Epoch: 104 | LOSS: Train: 3.1959 Val: 3.8232 Test: 4.2664  | ROC: Train: 0.890, Val: 0.829, Test: 0.807\n",
            "Epoch: 105 | LOSS: Train: 3.1192 Val: 3.8508 Test: 4.2085  | ROC: Train: 0.897, Val: 0.832, Test: 0.820\n",
            "Epoch: 106 | LOSS: Train: 3.3419 Val: 3.8116 Test: 4.1784  | ROC: Train: 0.880, Val: 0.834, Test: 0.819\n",
            "Epoch: 107 | LOSS: Train: 3.1317 Val: 3.8487 Test: 4.2574  | ROC: Train: 0.896, Val: 0.834, Test: 0.815\n",
            "Epoch: 108 | LOSS: Train: 3.0867 Val: 3.8204 Test: 4.2248  | ROC: Train: 0.897, Val: 0.830, Test: 0.819\n",
            "Epoch: 109 | LOSS: Train: 3.0800 Val: 3.7871 Test: 4.1877  | ROC: Train: 0.900, Val: 0.834, Test: 0.818\n",
            "Epoch: 110 | LOSS: Train: 3.1093 Val: 3.9939 Test: 4.3919  | ROC: Train: 0.897, Val: 0.834, Test: 0.808\n",
            "Epoch: 111 | LOSS: Train: 3.0915 Val: 3.7965 Test: 4.1817  | ROC: Train: 0.903, Val: 0.830, Test: 0.823\n",
            "Epoch: 112 | LOSS: Train: 3.0899 Val: 3.7930 Test: 4.2205  | ROC: Train: 0.899, Val: 0.842, Test: 0.819\n",
            "Epoch: 113 | LOSS: Train: 3.0546 Val: 3.8368 Test: 4.2523  | ROC: Train: 0.903, Val: 0.840, Test: 0.812\n",
            "Epoch: 114 | LOSS: Train: 3.0634 Val: 3.8306 Test: 4.2244  | ROC: Train: 0.903, Val: 0.834, Test: 0.816\n",
            "Epoch: 115 | LOSS: Train: 3.0431 Val: 3.9245 Test: 4.3395  | ROC: Train: 0.906, Val: 0.830, Test: 0.815\n",
            "Epoch: 116 | LOSS: Train: 3.1106 Val: 3.8424 Test: 4.2383  | ROC: Train: 0.904, Val: 0.830, Test: 0.809\n",
            "Epoch: 117 | LOSS: Train: 3.0830 Val: 3.7867 Test: 4.1432  | ROC: Train: 0.898, Val: 0.826, Test: 0.813\n",
            "Epoch: 118 | LOSS: Train: 3.0551 Val: 4.1179 Test: 4.5643  | ROC: Train: 0.904, Val: 0.846, Test: 0.810\n",
            "Epoch: 119 | LOSS: Train: 3.0549 Val: 3.8578 Test: 4.2419  | ROC: Train: 0.907, Val: 0.820, Test: 0.816\n",
            "Epoch: 120 | LOSS: Train: 3.0277 Val: 3.8327 Test: 4.2207  | ROC: Train: 0.909, Val: 0.824, Test: 0.814\n",
            "Epoch: 121 | LOSS: Train: 2.9953 Val: 3.8720 Test: 4.3257  | ROC: Train: 0.910, Val: 0.835, Test: 0.800\n",
            "Epoch: 122 | LOSS: Train: 2.9849 Val: 4.2965 Test: 4.8075  | ROC: Train: 0.913, Val: 0.830, Test: 0.807\n",
            "Epoch: 123 | LOSS: Train: 3.0569 Val: 3.8302 Test: 4.2226  | ROC: Train: 0.905, Val: 0.818, Test: 0.816\n",
            "Epoch: 124 | LOSS: Train: 2.9647 Val: 4.0840 Test: 4.4378  | ROC: Train: 0.915, Val: 0.824, Test: 0.813\n",
            "Epoch: 125 | LOSS: Train: 2.9845 Val: 4.0116 Test: 4.4568  | ROC: Train: 0.912, Val: 0.824, Test: 0.797\n",
            "Epoch: 126 | LOSS: Train: 2.9885 Val: 3.9808 Test: 4.4342  | ROC: Train: 0.912, Val: 0.809, Test: 0.797\n",
            "Epoch: 127 | LOSS: Train: 2.9264 Val: 4.0688 Test: 4.4958  | ROC: Train: 0.918, Val: 0.829, Test: 0.823\n",
            "Epoch: 128 | LOSS: Train: 2.9903 Val: 3.9755 Test: 4.4413  | ROC: Train: 0.914, Val: 0.827, Test: 0.807\n",
            "Epoch: 129 | LOSS: Train: 2.9219 Val: 3.8831 Test: 4.2776  | ROC: Train: 0.918, Val: 0.825, Test: 0.814\n",
            "Epoch: 130 | LOSS: Train: 2.9054 Val: 3.8295 Test: 4.2142  | ROC: Train: 0.917, Val: 0.814, Test: 0.807\n",
            "Epoch: 131 | LOSS: Train: 2.9263 Val: 4.2000 Test: 4.6665  | ROC: Train: 0.917, Val: 0.834, Test: 0.801\n",
            "Epoch: 132 | LOSS: Train: 2.9153 Val: 3.9050 Test: 4.3086  | ROC: Train: 0.920, Val: 0.817, Test: 0.804\n",
            "Epoch: 133 | LOSS: Train: 2.9036 Val: 4.0314 Test: 4.4673  | ROC: Train: 0.923, Val: 0.828, Test: 0.806\n",
            "Epoch: 134 | LOSS: Train: 2.9550 Val: 4.5077 Test: 4.8819  | ROC: Train: 0.918, Val: 0.830, Test: 0.812\n",
            "Epoch: 135 | LOSS: Train: 2.9560 Val: 3.8459 Test: 4.2780  | ROC: Train: 0.915, Val: 0.820, Test: 0.789\n",
            "Epoch: 136 | LOSS: Train: 2.9226 Val: 4.0306 Test: 4.4063  | ROC: Train: 0.920, Val: 0.819, Test: 0.798\n",
            "Epoch: 137 | LOSS: Train: 2.8558 Val: 3.9128 Test: 4.3246  | ROC: Train: 0.923, Val: 0.819, Test: 0.811\n",
            "Epoch: 138 | LOSS: Train: 2.8698 Val: 4.0540 Test: 4.3933  | ROC: Train: 0.925, Val: 0.816, Test: 0.799\n",
            "Epoch: 139 | LOSS: Train: 2.8427 Val: 3.9366 Test: 4.4502  | ROC: Train: 0.923, Val: 0.817, Test: 0.789\n",
            "Epoch: 140 | LOSS: Train: 2.8811 Val: 3.9930 Test: 4.4596  | ROC: Train: 0.924, Val: 0.819, Test: 0.802\n",
            "Epoch: 141 | LOSS: Train: 2.8660 Val: 3.9308 Test: 4.4556  | ROC: Train: 0.925, Val: 0.814, Test: 0.793\n",
            "Epoch: 142 | LOSS: Train: 2.8941 Val: 3.8738 Test: 4.3095  | ROC: Train: 0.928, Val: 0.814, Test: 0.800\n",
            "Epoch: 143 | LOSS: Train: 2.7983 Val: 4.0086 Test: 4.4457  | ROC: Train: 0.930, Val: 0.814, Test: 0.799\n",
            "Epoch: 144 | LOSS: Train: 2.8258 Val: 3.9981 Test: 4.3612  | ROC: Train: 0.927, Val: 0.823, Test: 0.805\n",
            "Epoch: 145 | LOSS: Train: 2.7416 Val: 4.2200 Test: 4.6302  | ROC: Train: 0.935, Val: 0.796, Test: 0.792\n",
            "Epoch: 146 | LOSS: Train: 2.8532 Val: 4.4080 Test: 4.9157  | ROC: Train: 0.924, Val: 0.815, Test: 0.795\n",
            "Epoch: 147 | LOSS: Train: 2.8286 Val: 3.8369 Test: 4.2950  | ROC: Train: 0.928, Val: 0.814, Test: 0.795\n",
            "Epoch: 148 | LOSS: Train: 2.7580 Val: 4.1326 Test: 4.5781  | ROC: Train: 0.935, Val: 0.809, Test: 0.791\n",
            "Epoch: 149 | LOSS: Train: 2.7264 Val: 3.9809 Test: 4.5031  | ROC: Train: 0.936, Val: 0.805, Test: 0.798\n",
            "Epoch: 150 | LOSS: Train: 2.7518 Val: 3.9144 Test: 4.4260  | ROC: Train: 0.933, Val: 0.811, Test: 0.789\n",
            "Epoch: 151 | LOSS: Train: 2.7087 Val: 3.9866 Test: 4.4784  | ROC: Train: 0.936, Val: 0.806, Test: 0.792\n",
            "Epoch: 152 | LOSS: Train: 2.7896 Val: 3.9341 Test: 4.3947  | ROC: Train: 0.934, Val: 0.809, Test: 0.784\n",
            "Epoch: 153 | LOSS: Train: 2.6747 Val: 4.1216 Test: 4.5584  | ROC: Train: 0.942, Val: 0.809, Test: 0.789\n",
            "Epoch: 154 | LOSS: Train: 2.7528 Val: 4.0974 Test: 4.5780  | ROC: Train: 0.933, Val: 0.797, Test: 0.790\n",
            "Epoch: 155 | LOSS: Train: 2.6453 Val: 4.0528 Test: 4.4610  | ROC: Train: 0.944, Val: 0.809, Test: 0.795\n",
            "Epoch: 156 | LOSS: Train: 2.6922 Val: 4.3037 Test: 4.7474  | ROC: Train: 0.942, Val: 0.817, Test: 0.796\n",
            "Epoch: 157 | LOSS: Train: 2.6651 Val: 4.0460 Test: 4.5197  | ROC: Train: 0.940, Val: 0.805, Test: 0.784\n",
            "Epoch: 158 | LOSS: Train: 2.7394 Val: 4.2912 Test: 5.3499  | ROC: Train: 0.943, Val: 0.809, Test: 0.787\n",
            "Epoch: 159 | LOSS: Train: 2.7163 Val: 4.0107 Test: 4.4639  | ROC: Train: 0.942, Val: 0.809, Test: 0.790\n",
            "Epoch: 160 | LOSS: Train: 2.6291 Val: 4.0897 Test: 4.5673  | ROC: Train: 0.948, Val: 0.806, Test: 0.793\n",
            "Epoch: 161 | LOSS: Train: 2.6016 Val: 4.0201 Test: 4.5512  | ROC: Train: 0.947, Val: 0.804, Test: 0.778\n",
            "Epoch: 162 | LOSS: Train: 2.6158 Val: 4.7694 Test: 4.7641  | ROC: Train: 0.947, Val: 0.810, Test: 0.774\n",
            "Epoch: 163 | LOSS: Train: 2.7163 Val: 4.5091 Test: 5.0344  | ROC: Train: 0.942, Val: 0.802, Test: 0.784\n",
            "Epoch: 164 | LOSS: Train: 2.6413 Val: 4.0647 Test: 4.5302  | ROC: Train: 0.947, Val: 0.804, Test: 0.791\n",
            "Epoch: 165 | LOSS: Train: 2.6107 Val: 4.8978 Test: 4.7407  | ROC: Train: 0.946, Val: 0.816, Test: 0.801\n",
            "Epoch: 166 | LOSS: Train: 2.6307 Val: 4.0092 Test: 4.4964  | ROC: Train: 0.947, Val: 0.813, Test: 0.794\n",
            "Epoch: 167 | LOSS: Train: 2.5533 Val: 4.1409 Test: 4.6192  | ROC: Train: 0.951, Val: 0.808, Test: 0.796\n",
            "Epoch: 168 | LOSS: Train: 2.5614 Val: 4.1542 Test: 4.6376  | ROC: Train: 0.951, Val: 0.799, Test: 0.791\n",
            "Epoch: 169 | LOSS: Train: 2.5487 Val: 4.8110 Test: 4.7173  | ROC: Train: 0.952, Val: 0.793, Test: 0.780\n",
            "Epoch: 170 | LOSS: Train: 2.5147 Val: 4.1380 Test: 4.6418  | ROC: Train: 0.953, Val: 0.796, Test: 0.780\n",
            "Epoch: 171 | LOSS: Train: 2.6031 Val: 5.0111 Test: 5.3565  | ROC: Train: 0.950, Val: 0.814, Test: 0.775\n",
            "Epoch: 172 | LOSS: Train: 2.5112 Val: 4.2210 Test: 4.6812  | ROC: Train: 0.956, Val: 0.796, Test: 0.778\n",
            "Epoch: 173 | LOSS: Train: 2.5018 Val: 4.7834 Test: 4.7412  | ROC: Train: 0.956, Val: 0.813, Test: 0.790\n",
            "Epoch: 174 | LOSS: Train: 2.4510 Val: 4.8229 Test: 4.8562  | ROC: Train: 0.958, Val: 0.794, Test: 0.770\n",
            "Epoch: 175 | LOSS: Train: 2.4675 Val: 4.8649 Test: 4.7770  | ROC: Train: 0.958, Val: 0.804, Test: 0.781\n",
            "Epoch: 176 | LOSS: Train: 2.6019 Val: 4.2426 Test: 4.7644  | ROC: Train: 0.950, Val: 0.801, Test: 0.780\n",
            "Epoch: 177 | LOSS: Train: 2.4560 Val: 4.8957 Test: 4.8559  | ROC: Train: 0.958, Val: 0.798, Test: 0.791\n",
            "Epoch: 178 | LOSS: Train: 2.4993 Val: 4.3117 Test: 4.7205  | ROC: Train: 0.957, Val: 0.779, Test: 0.780\n",
            "Epoch: 179 | LOSS: Train: 2.5028 Val: 4.3104 Test: 4.8696  | ROC: Train: 0.955, Val: 0.811, Test: 0.797\n",
            "Epoch: 180 | LOSS: Train: 2.4981 Val: 5.1247 Test: 5.1149  | ROC: Train: 0.962, Val: 0.809, Test: 0.793\n",
            "Epoch: 181 | LOSS: Train: 2.4711 Val: 5.0904 Test: 5.0035  | ROC: Train: 0.958, Val: 0.801, Test: 0.791\n",
            "Epoch: 182 | LOSS: Train: 2.4618 Val: 5.0453 Test: 4.9539  | ROC: Train: 0.960, Val: 0.793, Test: 0.777\n",
            "Epoch: 183 | LOSS: Train: 2.3872 Val: 4.2557 Test: 4.7978  | ROC: Train: 0.961, Val: 0.801, Test: 0.783\n",
            "Epoch: 184 | LOSS: Train: 2.3513 Val: 4.1779 Test: 4.7188  | ROC: Train: 0.962, Val: 0.799, Test: 0.783\n",
            "Epoch: 185 | LOSS: Train: 2.3131 Val: 4.4467 Test: 4.9852  | ROC: Train: 0.965, Val: 0.800, Test: 0.776\n",
            "Epoch: 186 | LOSS: Train: 2.3529 Val: 4.9330 Test: 4.8287  | ROC: Train: 0.964, Val: 0.809, Test: 0.791\n",
            "Epoch: 187 | LOSS: Train: 2.3179 Val: 4.4337 Test: 5.0179  | ROC: Train: 0.967, Val: 0.788, Test: 0.768\n",
            "Epoch: 188 | LOSS: Train: 2.3915 Val: 4.4803 Test: 5.0614  | ROC: Train: 0.964, Val: 0.795, Test: 0.774\n",
            "Epoch: 189 | LOSS: Train: 2.4460 Val: 4.3745 Test: 4.7786  | ROC: Train: 0.961, Val: 0.777, Test: 0.775\n",
            "Epoch: 190 | LOSS: Train: 2.4337 Val: 4.3270 Test: 4.8310  | ROC: Train: 0.962, Val: 0.788, Test: 0.768\n",
            "Epoch: 191 | LOSS: Train: 2.3034 Val: 4.9062 Test: 4.9499  | ROC: Train: 0.967, Val: 0.791, Test: 0.770\n",
            "Epoch: 192 | LOSS: Train: 2.2643 Val: 4.6086 Test: 5.2157  | ROC: Train: 0.968, Val: 0.784, Test: 0.771\n",
            "Epoch: 193 | LOSS: Train: 2.2549 Val: 4.5157 Test: 4.9931  | ROC: Train: 0.970, Val: 0.798, Test: 0.777\n",
            "Epoch: 194 | LOSS: Train: 2.3156 Val: 5.3358 Test: 5.3280  | ROC: Train: 0.967, Val: 0.794, Test: 0.777\n",
            "Epoch: 195 | LOSS: Train: 2.2708 Val: 5.2621 Test: 5.1932  | ROC: Train: 0.971, Val: 0.784, Test: 0.786\n",
            "Epoch: 196 | LOSS: Train: 2.2807 Val: 5.1578 Test: 5.0959  | ROC: Train: 0.967, Val: 0.798, Test: 0.785\n",
            "Epoch: 197 | LOSS: Train: 2.2588 Val: 4.3687 Test: 4.8929  | ROC: Train: 0.970, Val: 0.787, Test: 0.783\n",
            "Epoch: 198 | LOSS: Train: 2.2800 Val: 5.1122 Test: 4.9819  | ROC: Train: 0.967, Val: 0.786, Test: 0.781\n",
            "Epoch: 199 | LOSS: Train: 2.2508 Val: 4.4414 Test: 5.0025  | ROC: Train: 0.970, Val: 0.788, Test: 0.769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = f\"/content/drive/MyDrive/AI/covid-1/covid-GraphConv-3x-3GRU--200-d02--1.pt\"\n",
        "torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': train_loss,\n",
        "        }, PATH)\n",
        "torch.save(model,  PATH.strip(\".pt\") + \"-model1.pt\" )"
      ],
      "metadata": {
        "id": "zwEbJ5LkeMhN"
      },
      "id": "zwEbJ5LkeMhN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the GraphSage Network"
      ],
      "metadata": {
        "id": "3iodKB3uXFgY"
      },
      "id": "3iodKB3uXFgY"
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(Net, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GraphSAGE(44, hidden_channels, num_layers=4) \n",
        "        # self.conv1 = GraphConv(44, hidden_channels)\n",
        "        # self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        # self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        # x = self.conv2(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        # x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "        x = global_add_pool(x, batch) \n",
        "        \n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        # x = x.sigmoid()\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = Net(hidden_channels=250)\n",
        "print(model)\n",
        "\n",
        "WEIGHT = 230\n",
        "print_every = 0\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=10**-3.5,\n",
        "                             weight_decay=10**-2.9)\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    train_loss, train_roc = train()\n",
        "    val_loss, val_roc = test(val_loader)\n",
        "    test_loss, test_roc = test(test_loader)\n",
        "    # train_loss, train_roc = test(train_loader)\n",
        "    print(f'Epoch: {epoch:03d} | LOSS: Train: {train_loss:.4f} Val: {val_loss:.4f} Test: {test_loss:.4f}  | ROC: Train: {train_roc:.3f}, Val: {val_roc:.3f}, '\n",
        "          f'Test: {test_roc:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvmUtapXMpGA",
        "outputId": "71224c83-0c2b-4158-8c33-8f0bdd18aa3e"
      },
      "id": "QvmUtapXMpGA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): GraphSAGE(44, 250, num_layers=4)\n",
            "  (lin): Linear(in_features=250, out_features=1, bias=True)\n",
            ")\n",
            "Epoch: 001 | LOSS: Train: 14.7445 Val: 7.1723 Test: 6.8976  | ROC: Train: 0.384, Val: 0.271, Test: 0.364\n",
            "Epoch: 002 | LOSS: Train: 7.4244 Val: 7.0724 Test: 6.7874  | ROC: Train: 0.358, Val: 0.299, Test: 0.404\n",
            "Epoch: 003 | LOSS: Train: 7.0092 Val: 6.8574 Test: 6.6405  | ROC: Train: 0.424, Val: 0.360, Test: 0.449\n",
            "Epoch: 004 | LOSS: Train: 6.7787 Val: 6.5842 Test: 6.4652  | ROC: Train: 0.474, Val: 0.431, Test: 0.489\n",
            "Epoch: 005 | LOSS: Train: 6.6524 Val: 6.4293 Test: 6.3346  | ROC: Train: 0.508, Val: 0.455, Test: 0.502\n",
            "Epoch: 006 | LOSS: Train: 6.3423 Val: 6.4724 Test: 6.4509  | ROC: Train: 0.560, Val: 0.490, Test: 0.510\n",
            "Epoch: 007 | LOSS: Train: 6.2522 Val: 6.1250 Test: 6.1633  | ROC: Train: 0.570, Val: 0.521, Test: 0.535\n",
            "Epoch: 008 | LOSS: Train: 6.0589 Val: 5.9030 Test: 6.0261  | ROC: Train: 0.602, Val: 0.557, Test: 0.564\n",
            "Epoch: 009 | LOSS: Train: 5.9684 Val: 5.7980 Test: 5.9574  | ROC: Train: 0.608, Val: 0.591, Test: 0.594\n",
            "Epoch: 010 | LOSS: Train: 5.9112 Val: 5.8798 Test: 6.0322  | ROC: Train: 0.626, Val: 0.574, Test: 0.577\n",
            "Epoch: 011 | LOSS: Train: 5.8768 Val: 5.6992 Test: 5.8866  | ROC: Train: 0.623, Val: 0.586, Test: 0.590\n",
            "Epoch: 012 | LOSS: Train: 5.6955 Val: 6.0812 Test: 6.2209  | ROC: Train: 0.646, Val: 0.578, Test: 0.587\n",
            "Epoch: 013 | LOSS: Train: 5.6462 Val: 5.6088 Test: 5.7804  | ROC: Train: 0.656, Val: 0.606, Test: 0.614\n",
            "Epoch: 014 | LOSS: Train: 5.6483 Val: 5.6578 Test: 5.8780  | ROC: Train: 0.658, Val: 0.644, Test: 0.640\n",
            "Epoch: 015 | LOSS: Train: 5.6026 Val: 5.7219 Test: 5.9195  | ROC: Train: 0.671, Val: 0.600, Test: 0.603\n",
            "Epoch: 016 | LOSS: Train: 5.5651 Val: 5.5088 Test: 5.7380  | ROC: Train: 0.661, Val: 0.620, Test: 0.622\n",
            "Epoch: 017 | LOSS: Train: 5.4698 Val: 5.3910 Test: 5.5825  | ROC: Train: 0.681, Val: 0.642, Test: 0.642\n",
            "Epoch: 018 | LOSS: Train: 5.3327 Val: 5.3713 Test: 5.6045  | ROC: Train: 0.694, Val: 0.647, Test: 0.645\n",
            "Epoch: 019 | LOSS: Train: 5.2780 Val: 5.3216 Test: 5.4929  | ROC: Train: 0.704, Val: 0.663, Test: 0.663\n",
            "Epoch: 020 | LOSS: Train: 5.4262 Val: 5.3070 Test: 5.5441  | ROC: Train: 0.686, Val: 0.661, Test: 0.657\n",
            "Epoch: 021 | LOSS: Train: 5.3257 Val: 5.2510 Test: 5.4819  | ROC: Train: 0.698, Val: 0.659, Test: 0.655\n",
            "Epoch: 022 | LOSS: Train: 5.1433 Val: 5.1395 Test: 5.3420  | ROC: Train: 0.719, Val: 0.674, Test: 0.674\n",
            "Epoch: 023 | LOSS: Train: 5.1804 Val: 5.2386 Test: 5.4375  | ROC: Train: 0.715, Val: 0.660, Test: 0.661\n",
            "Epoch: 024 | LOSS: Train: 5.0719 Val: 5.1082 Test: 5.3357  | ROC: Train: 0.724, Val: 0.678, Test: 0.679\n",
            "Epoch: 025 | LOSS: Train: 4.9895 Val: 5.0678 Test: 5.2614  | ROC: Train: 0.738, Val: 0.696, Test: 0.698\n",
            "Epoch: 026 | LOSS: Train: 5.0142 Val: 5.1174 Test: 5.3505  | ROC: Train: 0.736, Val: 0.689, Test: 0.685\n",
            "Epoch: 027 | LOSS: Train: 4.9961 Val: 5.1502 Test: 5.3526  | ROC: Train: 0.736, Val: 0.680, Test: 0.684\n",
            "Epoch: 028 | LOSS: Train: 4.9368 Val: 5.2407 Test: 5.4262  | ROC: Train: 0.737, Val: 0.665, Test: 0.670\n",
            "Epoch: 029 | LOSS: Train: 4.9487 Val: 5.1388 Test: 5.3640  | ROC: Train: 0.733, Val: 0.680, Test: 0.684\n",
            "Epoch: 030 | LOSS: Train: 4.9566 Val: 4.9771 Test: 5.1463  | ROC: Train: 0.738, Val: 0.690, Test: 0.698\n",
            "Epoch: 031 | LOSS: Train: 4.9031 Val: 4.9590 Test: 5.2074  | ROC: Train: 0.739, Val: 0.691, Test: 0.684\n",
            "Epoch: 032 | LOSS: Train: 4.9010 Val: 5.0488 Test: 5.2670  | ROC: Train: 0.739, Val: 0.676, Test: 0.676\n",
            "Epoch: 033 | LOSS: Train: 4.8455 Val: 4.9943 Test: 5.2004  | ROC: Train: 0.748, Val: 0.695, Test: 0.696\n",
            "Epoch: 034 | LOSS: Train: 4.7673 Val: 4.8632 Test: 5.0801  | ROC: Train: 0.752, Val: 0.699, Test: 0.699\n",
            "Epoch: 035 | LOSS: Train: 4.7438 Val: 5.0241 Test: 5.2245  | ROC: Train: 0.751, Val: 0.680, Test: 0.685\n",
            "Epoch: 036 | LOSS: Train: 4.7099 Val: 5.1387 Test: 5.3255  | ROC: Train: 0.759, Val: 0.674, Test: 0.682\n",
            "Epoch: 037 | LOSS: Train: 4.7032 Val: 4.8473 Test: 5.0728  | ROC: Train: 0.752, Val: 0.699, Test: 0.700\n",
            "Epoch: 038 | LOSS: Train: 4.5998 Val: 5.2224 Test: 5.4746  | ROC: Train: 0.770, Val: 0.738, Test: 0.728\n",
            "Epoch: 039 | LOSS: Train: 4.6461 Val: 4.8577 Test: 5.0656  | ROC: Train: 0.768, Val: 0.708, Test: 0.704\n",
            "Epoch: 040 | LOSS: Train: 4.6667 Val: 4.9789 Test: 5.2097  | ROC: Train: 0.767, Val: 0.687, Test: 0.689\n",
            "Epoch: 041 | LOSS: Train: 4.6364 Val: 4.8642 Test: 5.0803  | ROC: Train: 0.767, Val: 0.715, Test: 0.712\n",
            "Epoch: 042 | LOSS: Train: 4.5869 Val: 4.8532 Test: 5.0351  | ROC: Train: 0.766, Val: 0.701, Test: 0.707\n",
            "Epoch: 043 | LOSS: Train: 4.5311 Val: 5.0141 Test: 5.2449  | ROC: Train: 0.775, Val: 0.699, Test: 0.697\n",
            "Epoch: 044 | LOSS: Train: 4.4630 Val: 5.0762 Test: 5.3179  | ROC: Train: 0.782, Val: 0.740, Test: 0.734\n",
            "Epoch: 045 | LOSS: Train: 4.7055 Val: 4.7589 Test: 4.9841  | ROC: Train: 0.759, Val: 0.715, Test: 0.715\n",
            "Epoch: 046 | LOSS: Train: 4.3821 Val: 4.8219 Test: 5.0968  | ROC: Train: 0.786, Val: 0.717, Test: 0.715\n",
            "Epoch: 047 | LOSS: Train: 4.5238 Val: 4.8330 Test: 5.0368  | ROC: Train: 0.773, Val: 0.704, Test: 0.713\n",
            "Epoch: 048 | LOSS: Train: 4.4810 Val: 4.7688 Test: 4.9688  | ROC: Train: 0.776, Val: 0.711, Test: 0.715\n",
            "Epoch: 049 | LOSS: Train: 4.4320 Val: 5.2599 Test: 5.4450  | ROC: Train: 0.780, Val: 0.735, Test: 0.730\n",
            "Epoch: 050 | LOSS: Train: 4.4158 Val: 4.7094 Test: 4.9380  | ROC: Train: 0.782, Val: 0.713, Test: 0.717\n",
            "Epoch: 051 | LOSS: Train: 4.3835 Val: 4.7863 Test: 4.9894  | ROC: Train: 0.790, Val: 0.735, Test: 0.739\n",
            "Epoch: 052 | LOSS: Train: 4.4024 Val: 4.6770 Test: 4.9207  | ROC: Train: 0.788, Val: 0.730, Test: 0.727\n",
            "Epoch: 053 | LOSS: Train: 4.3415 Val: 4.7640 Test: 4.9530  | ROC: Train: 0.791, Val: 0.735, Test: 0.736\n",
            "Epoch: 054 | LOSS: Train: 4.2429 Val: 4.6473 Test: 4.9186  | ROC: Train: 0.801, Val: 0.739, Test: 0.734\n",
            "Epoch: 055 | LOSS: Train: 4.2925 Val: 4.9187 Test: 5.1347  | ROC: Train: 0.799, Val: 0.713, Test: 0.714\n",
            "Epoch: 056 | LOSS: Train: 4.2741 Val: 5.0171 Test: 5.2198  | ROC: Train: 0.800, Val: 0.708, Test: 0.710\n",
            "Epoch: 057 | LOSS: Train: 4.2853 Val: 4.6566 Test: 4.9099  | ROC: Train: 0.800, Val: 0.733, Test: 0.732\n",
            "Epoch: 058 | LOSS: Train: 4.2012 Val: 5.0771 Test: 5.3258  | ROC: Train: 0.806, Val: 0.758, Test: 0.754\n",
            "Epoch: 059 | LOSS: Train: 4.4421 Val: 4.6380 Test: 4.8506  | ROC: Train: 0.782, Val: 0.718, Test: 0.721\n",
            "Epoch: 060 | LOSS: Train: 4.2933 Val: 5.2000 Test: 5.4719  | ROC: Train: 0.796, Val: 0.709, Test: 0.707\n",
            "Epoch: 061 | LOSS: Train: 4.1028 Val: 4.6770 Test: 4.8812  | ROC: Train: 0.811, Val: 0.736, Test: 0.727\n",
            "Epoch: 062 | LOSS: Train: 4.2035 Val: 4.9329 Test: 5.1598  | ROC: Train: 0.809, Val: 0.720, Test: 0.716\n",
            "Epoch: 063 | LOSS: Train: 4.1138 Val: 4.6927 Test: 4.9531  | ROC: Train: 0.818, Val: 0.757, Test: 0.750\n",
            "Epoch: 064 | LOSS: Train: 4.0944 Val: 4.5426 Test: 4.8053  | ROC: Train: 0.816, Val: 0.750, Test: 0.741\n",
            "Epoch: 065 | LOSS: Train: 4.0191 Val: 4.6320 Test: 4.8177  | ROC: Train: 0.828, Val: 0.752, Test: 0.751\n",
            "Epoch: 066 | LOSS: Train: 4.0552 Val: 4.5066 Test: 4.8103  | ROC: Train: 0.822, Val: 0.754, Test: 0.732\n",
            "Epoch: 067 | LOSS: Train: 3.9813 Val: 4.6082 Test: 4.8137  | ROC: Train: 0.827, Val: 0.750, Test: 0.741\n",
            "Epoch: 068 | LOSS: Train: 3.9509 Val: 4.6683 Test: 4.8384  | ROC: Train: 0.831, Val: 0.755, Test: 0.753\n",
            "Epoch: 069 | LOSS: Train: 4.1545 Val: 4.5511 Test: 4.7656  | ROC: Train: 0.816, Val: 0.746, Test: 0.739\n",
            "Epoch: 070 | LOSS: Train: 3.9321 Val: 4.6438 Test: 4.8279  | ROC: Train: 0.835, Val: 0.741, Test: 0.744\n",
            "Epoch: 071 | LOSS: Train: 4.0824 Val: 4.5320 Test: 4.7554  | ROC: Train: 0.820, Val: 0.746, Test: 0.734\n",
            "Epoch: 072 | LOSS: Train: 3.9999 Val: 4.5523 Test: 4.7884  | ROC: Train: 0.833, Val: 0.751, Test: 0.739\n",
            "Epoch: 073 | LOSS: Train: 3.8852 Val: 4.6700 Test: 4.8830  | ROC: Train: 0.841, Val: 0.758, Test: 0.748\n",
            "Epoch: 074 | LOSS: Train: 3.9179 Val: 4.7287 Test: 4.9883  | ROC: Train: 0.829, Val: 0.746, Test: 0.732\n",
            "Epoch: 075 | LOSS: Train: 3.8904 Val: 4.5095 Test: 4.7675  | ROC: Train: 0.841, Val: 0.759, Test: 0.743\n",
            "Epoch: 076 | LOSS: Train: 3.8611 Val: 4.5040 Test: 4.8338  | ROC: Train: 0.843, Val: 0.749, Test: 0.728\n",
            "Epoch: 077 | LOSS: Train: 3.8289 Val: 4.6137 Test: 4.7079  | ROC: Train: 0.845, Val: 0.747, Test: 0.745\n",
            "Epoch: 078 | LOSS: Train: 3.8784 Val: 4.6204 Test: 4.9102  | ROC: Train: 0.846, Val: 0.750, Test: 0.735\n",
            "Epoch: 079 | LOSS: Train: 3.7860 Val: 4.8476 Test: 5.0657  | ROC: Train: 0.854, Val: 0.741, Test: 0.729\n",
            "Epoch: 080 | LOSS: Train: 3.9458 Val: 4.4995 Test: 4.6600  | ROC: Train: 0.840, Val: 0.742, Test: 0.743\n",
            "Epoch: 081 | LOSS: Train: 3.7813 Val: 4.6700 Test: 4.8420  | ROC: Train: 0.848, Val: 0.756, Test: 0.748\n",
            "Epoch: 082 | LOSS: Train: 3.7094 Val: 4.5822 Test: 4.8534  | ROC: Train: 0.853, Val: 0.760, Test: 0.744\n",
            "Epoch: 083 | LOSS: Train: 3.7435 Val: 4.6240 Test: 4.8747  | ROC: Train: 0.854, Val: 0.750, Test: 0.740\n",
            "Epoch: 084 | LOSS: Train: 3.7154 Val: 4.6386 Test: 4.8745  | ROC: Train: 0.854, Val: 0.766, Test: 0.748\n",
            "Epoch: 085 | LOSS: Train: 3.8403 Val: 4.5101 Test: 4.7686  | ROC: Train: 0.844, Val: 0.771, Test: 0.750\n",
            "Epoch: 086 | LOSS: Train: 3.7411 Val: 4.5570 Test: 4.7953  | ROC: Train: 0.850, Val: 0.765, Test: 0.747\n",
            "Epoch: 087 | LOSS: Train: 3.6515 Val: 4.6480 Test: 4.8207  | ROC: Train: 0.860, Val: 0.759, Test: 0.750\n",
            "Epoch: 088 | LOSS: Train: 3.6310 Val: 4.5057 Test: 4.6572  | ROC: Train: 0.862, Val: 0.764, Test: 0.751\n",
            "Epoch: 089 | LOSS: Train: 3.6979 Val: 4.5827 Test: 4.7204  | ROC: Train: 0.861, Val: 0.757, Test: 0.749\n",
            "Epoch: 090 | LOSS: Train: 3.6557 Val: 4.4571 Test: 4.6778  | ROC: Train: 0.858, Val: 0.758, Test: 0.748\n",
            "Epoch: 091 | LOSS: Train: 3.7517 Val: 4.5153 Test: 4.6527  | ROC: Train: 0.854, Val: 0.770, Test: 0.761\n",
            "Epoch: 092 | LOSS: Train: 3.6472 Val: 4.6277 Test: 4.7448  | ROC: Train: 0.864, Val: 0.760, Test: 0.747\n",
            "Epoch: 093 | LOSS: Train: 3.5543 Val: 4.6476 Test: 4.8649  | ROC: Train: 0.873, Val: 0.762, Test: 0.754\n",
            "Epoch: 094 | LOSS: Train: 3.5370 Val: 4.6585 Test: 4.8815  | ROC: Train: 0.873, Val: 0.759, Test: 0.744\n",
            "Epoch: 095 | LOSS: Train: 3.4777 Val: 4.5231 Test: 4.8317  | ROC: Train: 0.871, Val: 0.775, Test: 0.759\n",
            "Epoch: 096 | LOSS: Train: 3.4759 Val: 5.5471 Test: 5.2047  | ROC: Train: 0.877, Val: 0.775, Test: 0.744\n",
            "Epoch: 097 | LOSS: Train: 3.5557 Val: 4.5962 Test: 4.8042  | ROC: Train: 0.871, Val: 0.772, Test: 0.755\n",
            "Epoch: 098 | LOSS: Train: 3.4917 Val: 4.6602 Test: 4.9434  | ROC: Train: 0.879, Val: 0.759, Test: 0.744\n",
            "Epoch: 099 | LOSS: Train: 3.4896 Val: 4.9299 Test: 5.1667  | ROC: Train: 0.878, Val: 0.759, Test: 0.750\n",
            "Epoch: 100 | LOSS: Train: 3.4774 Val: 4.5503 Test: 4.8015  | ROC: Train: 0.877, Val: 0.772, Test: 0.755\n",
            "Epoch: 101 | LOSS: Train: 3.4434 Val: 4.5062 Test: 4.7402  | ROC: Train: 0.877, Val: 0.785, Test: 0.762\n",
            "Epoch: 102 | LOSS: Train: 3.3904 Val: 4.5461 Test: 4.7502  | ROC: Train: 0.887, Val: 0.769, Test: 0.756\n",
            "Epoch: 103 | LOSS: Train: 3.6958 Val: 5.2082 Test: 4.8818  | ROC: Train: 0.864, Val: 0.773, Test: 0.758\n",
            "Epoch: 104 | LOSS: Train: 3.4487 Val: 4.5357 Test: 4.8018  | ROC: Train: 0.878, Val: 0.776, Test: 0.752\n",
            "Epoch: 105 | LOSS: Train: 3.3647 Val: 4.4447 Test: 4.7307  | ROC: Train: 0.886, Val: 0.768, Test: 0.753\n",
            "Epoch: 106 | LOSS: Train: 3.3807 Val: 5.3649 Test: 5.0719  | ROC: Train: 0.886, Val: 0.787, Test: 0.759\n",
            "Epoch: 107 | LOSS: Train: 3.3295 Val: 4.5294 Test: 4.7448  | ROC: Train: 0.894, Val: 0.777, Test: 0.759\n",
            "Epoch: 108 | LOSS: Train: 3.3047 Val: 4.8362 Test: 5.0594  | ROC: Train: 0.895, Val: 0.757, Test: 0.745\n",
            "Epoch: 109 | LOSS: Train: 3.3718 Val: 4.8043 Test: 5.1181  | ROC: Train: 0.891, Val: 0.780, Test: 0.759\n",
            "Epoch: 110 | LOSS: Train: 3.3678 Val: 4.8873 Test: 5.1169  | ROC: Train: 0.895, Val: 0.764, Test: 0.749\n",
            "Epoch: 111 | LOSS: Train: 3.4635 Val: 4.4243 Test: 4.6853  | ROC: Train: 0.883, Val: 0.774, Test: 0.755\n",
            "Epoch: 112 | LOSS: Train: 3.2661 Val: 4.5456 Test: 4.7558  | ROC: Train: 0.896, Val: 0.766, Test: 0.747\n",
            "Epoch: 113 | LOSS: Train: 3.2913 Val: 4.5235 Test: 4.7485  | ROC: Train: 0.897, Val: 0.773, Test: 0.757\n",
            "Epoch: 114 | LOSS: Train: 3.3928 Val: 4.7099 Test: 4.9750  | ROC: Train: 0.890, Val: 0.770, Test: 0.746\n",
            "Epoch: 115 | LOSS: Train: 3.3670 Val: 4.5634 Test: 4.8641  | ROC: Train: 0.888, Val: 0.779, Test: 0.750\n",
            "Epoch: 116 | LOSS: Train: 3.2480 Val: 4.6933 Test: 4.9478  | ROC: Train: 0.898, Val: 0.774, Test: 0.744\n",
            "Epoch: 117 | LOSS: Train: 3.1762 Val: 5.8310 Test: 5.4381  | ROC: Train: 0.905, Val: 0.780, Test: 0.761\n",
            "Epoch: 118 | LOSS: Train: 3.2359 Val: 4.4841 Test: 4.7774  | ROC: Train: 0.901, Val: 0.774, Test: 0.760\n",
            "Epoch: 119 | LOSS: Train: 3.1557 Val: 5.0009 Test: 5.3170  | ROC: Train: 0.905, Val: 0.764, Test: 0.748\n",
            "Epoch: 120 | LOSS: Train: 3.2851 Val: 4.6434 Test: 4.9118  | ROC: Train: 0.901, Val: 0.785, Test: 0.760\n",
            "Epoch: 121 | LOSS: Train: 3.2244 Val: 5.1703 Test: 5.5665  | ROC: Train: 0.903, Val: 0.760, Test: 0.745\n",
            "Epoch: 122 | LOSS: Train: 3.1834 Val: 5.6038 Test: 5.3287  | ROC: Train: 0.905, Val: 0.789, Test: 0.759\n",
            "Epoch: 123 | LOSS: Train: 3.2926 Val: 4.7367 Test: 4.8809  | ROC: Train: 0.901, Val: 0.764, Test: 0.751\n",
            "Epoch: 124 | LOSS: Train: 3.1262 Val: 4.9273 Test: 5.2029  | ROC: Train: 0.910, Val: 0.758, Test: 0.738\n",
            "Epoch: 125 | LOSS: Train: 3.1876 Val: 4.7744 Test: 5.1770  | ROC: Train: 0.905, Val: 0.771, Test: 0.751\n",
            "Epoch: 126 | LOSS: Train: 3.0639 Val: 5.2695 Test: 5.0263  | ROC: Train: 0.917, Val: 0.787, Test: 0.761\n",
            "Epoch: 127 | LOSS: Train: 3.1124 Val: 4.8117 Test: 5.0193  | ROC: Train: 0.910, Val: 0.765, Test: 0.753\n",
            "Epoch: 128 | LOSS: Train: 3.0979 Val: 4.6764 Test: 4.9564  | ROC: Train: 0.913, Val: 0.768, Test: 0.753\n",
            "Epoch: 129 | LOSS: Train: 3.1925 Val: 4.5908 Test: 4.9033  | ROC: Train: 0.912, Val: 0.783, Test: 0.755\n",
            "Epoch: 130 | LOSS: Train: 3.0429 Val: 4.5885 Test: 4.7623  | ROC: Train: 0.916, Val: 0.773, Test: 0.748\n",
            "Epoch: 131 | LOSS: Train: 3.0650 Val: 4.4576 Test: 4.7724  | ROC: Train: 0.918, Val: 0.780, Test: 0.755\n",
            "Epoch: 132 | LOSS: Train: 2.9760 Val: 4.6356 Test: 4.9581  | ROC: Train: 0.923, Val: 0.770, Test: 0.748\n",
            "Epoch: 133 | LOSS: Train: 3.0870 Val: 4.5319 Test: 4.9666  | ROC: Train: 0.916, Val: 0.780, Test: 0.750\n",
            "Epoch: 134 | LOSS: Train: 3.0226 Val: 4.5192 Test: 4.8979  | ROC: Train: 0.920, Val: 0.773, Test: 0.757\n",
            "Epoch: 135 | LOSS: Train: 3.1624 Val: 5.2943 Test: 4.9281  | ROC: Train: 0.911, Val: 0.781, Test: 0.759\n",
            "Epoch: 136 | LOSS: Train: 3.2038 Val: 5.4532 Test: 5.1949  | ROC: Train: 0.914, Val: 0.788, Test: 0.765\n",
            "Epoch: 137 | LOSS: Train: 2.9775 Val: 4.7532 Test: 4.9899  | ROC: Train: 0.925, Val: 0.777, Test: 0.760\n",
            "Epoch: 138 | LOSS: Train: 2.9395 Val: 5.3311 Test: 5.0778  | ROC: Train: 0.926, Val: 0.777, Test: 0.760\n",
            "Epoch: 139 | LOSS: Train: 2.9220 Val: 4.6308 Test: 5.0536  | ROC: Train: 0.928, Val: 0.775, Test: 0.752\n",
            "Epoch: 140 | LOSS: Train: 2.8760 Val: 4.7918 Test: 5.1088  | ROC: Train: 0.931, Val: 0.768, Test: 0.745\n",
            "Epoch: 141 | LOSS: Train: 2.8923 Val: 4.5392 Test: 4.8274  | ROC: Train: 0.929, Val: 0.778, Test: 0.763\n",
            "Epoch: 142 | LOSS: Train: 2.9269 Val: 6.0046 Test: 5.7820  | ROC: Train: 0.928, Val: 0.788, Test: 0.764\n",
            "Epoch: 143 | LOSS: Train: 3.0304 Val: 4.6185 Test: 4.9856  | ROC: Train: 0.922, Val: 0.786, Test: 0.755\n",
            "Epoch: 144 | LOSS: Train: 2.8065 Val: 5.8934 Test: 5.5733  | ROC: Train: 0.935, Val: 0.784, Test: 0.757\n",
            "Epoch: 145 | LOSS: Train: 2.9452 Val: 4.8988 Test: 5.2273  | ROC: Train: 0.929, Val: 0.776, Test: 0.765\n",
            "Epoch: 146 | LOSS: Train: 2.8860 Val: 4.9568 Test: 5.1280  | ROC: Train: 0.933, Val: 0.770, Test: 0.750\n",
            "Epoch: 147 | LOSS: Train: 2.8258 Val: 4.5827 Test: 4.9972  | ROC: Train: 0.932, Val: 0.789, Test: 0.761\n",
            "Epoch: 148 | LOSS: Train: 2.7803 Val: 5.1694 Test: 5.5040  | ROC: Train: 0.939, Val: 0.772, Test: 0.749\n",
            "Epoch: 149 | LOSS: Train: 2.9195 Val: 4.9648 Test: 5.2957  | ROC: Train: 0.932, Val: 0.770, Test: 0.747\n",
            "Epoch: 150 | LOSS: Train: 2.7684 Val: 5.1531 Test: 5.4558  | ROC: Train: 0.940, Val: 0.770, Test: 0.755\n",
            "Epoch: 151 | LOSS: Train: 2.7987 Val: 4.7600 Test: 4.9942  | ROC: Train: 0.939, Val: 0.781, Test: 0.764\n",
            "Epoch: 152 | LOSS: Train: 2.9984 Val: 4.6544 Test: 4.9312  | ROC: Train: 0.924, Val: 0.777, Test: 0.750\n",
            "Epoch: 153 | LOSS: Train: 2.7833 Val: 5.7348 Test: 5.4803  | ROC: Train: 0.939, Val: 0.778, Test: 0.753\n",
            "Epoch: 154 | LOSS: Train: 2.6887 Val: 5.5465 Test: 5.2545  | ROC: Train: 0.945, Val: 0.784, Test: 0.764\n",
            "Epoch: 155 | LOSS: Train: 2.6789 Val: 4.7598 Test: 5.1420  | ROC: Train: 0.945, Val: 0.781, Test: 0.754\n",
            "Epoch: 156 | LOSS: Train: 2.7270 Val: 4.6815 Test: 4.9790  | ROC: Train: 0.943, Val: 0.778, Test: 0.754\n",
            "Epoch: 157 | LOSS: Train: 2.7387 Val: 4.8131 Test: 5.1293  | ROC: Train: 0.941, Val: 0.771, Test: 0.746\n",
            "Epoch: 158 | LOSS: Train: 2.7577 Val: 4.8220 Test: 5.1795  | ROC: Train: 0.940, Val: 0.777, Test: 0.760\n",
            "Epoch: 159 | LOSS: Train: 2.6838 Val: 4.8655 Test: 5.1947  | ROC: Train: 0.947, Val: 0.777, Test: 0.760\n",
            "Epoch: 160 | LOSS: Train: 2.6949 Val: 5.5732 Test: 5.5395  | ROC: Train: 0.948, Val: 0.779, Test: 0.763\n",
            "Epoch: 161 | LOSS: Train: 2.7449 Val: 4.8070 Test: 5.2102  | ROC: Train: 0.944, Val: 0.774, Test: 0.750\n",
            "Epoch: 162 | LOSS: Train: 2.7671 Val: 4.7103 Test: 4.9221  | ROC: Train: 0.944, Val: 0.782, Test: 0.760\n",
            "Epoch: 163 | LOSS: Train: 2.8005 Val: 4.6277 Test: 4.9850  | ROC: Train: 0.942, Val: 0.779, Test: 0.753\n",
            "Epoch: 164 | LOSS: Train: 2.7038 Val: 4.6965 Test: 5.2736  | ROC: Train: 0.946, Val: 0.785, Test: 0.749\n",
            "Epoch: 165 | LOSS: Train: 2.6102 Val: 5.1326 Test: 5.7536  | ROC: Train: 0.949, Val: 0.769, Test: 0.740\n",
            "Epoch: 166 | LOSS: Train: 2.6603 Val: 4.9305 Test: 5.5473  | ROC: Train: 0.948, Val: 0.780, Test: 0.741\n",
            "Epoch: 167 | LOSS: Train: 2.6245 Val: 4.9517 Test: 5.3586  | ROC: Train: 0.951, Val: 0.778, Test: 0.751\n",
            "Epoch: 168 | LOSS: Train: 2.5694 Val: 4.8260 Test: 5.2448  | ROC: Train: 0.955, Val: 0.777, Test: 0.757\n",
            "Epoch: 169 | LOSS: Train: 2.7342 Val: 5.0281 Test: 5.4391  | ROC: Train: 0.949, Val: 0.782, Test: 0.761\n",
            "Epoch: 170 | LOSS: Train: 2.5347 Val: 4.9174 Test: 5.3233  | ROC: Train: 0.955, Val: 0.790, Test: 0.760\n",
            "Epoch: 171 | LOSS: Train: 2.6689 Val: 4.8211 Test: 5.1843  | ROC: Train: 0.951, Val: 0.767, Test: 0.759\n",
            "Epoch: 172 | LOSS: Train: 2.5203 Val: 4.8453 Test: 5.2913  | ROC: Train: 0.958, Val: 0.790, Test: 0.766\n",
            "Epoch: 173 | LOSS: Train: 2.5774 Val: 5.7132 Test: 5.6339  | ROC: Train: 0.955, Val: 0.785, Test: 0.754\n",
            "Epoch: 174 | LOSS: Train: 2.5276 Val: 5.1158 Test: 5.3574  | ROC: Train: 0.956, Val: 0.769, Test: 0.753\n",
            "Epoch: 175 | LOSS: Train: 2.5355 Val: 4.8908 Test: 5.4231  | ROC: Train: 0.953, Val: 0.776, Test: 0.747\n",
            "Epoch: 176 | LOSS: Train: 2.4356 Val: 5.6260 Test: 5.8284  | ROC: Train: 0.960, Val: 0.782, Test: 0.762\n",
            "Epoch: 177 | LOSS: Train: 2.4654 Val: 4.9662 Test: 5.5625  | ROC: Train: 0.960, Val: 0.782, Test: 0.752\n",
            "Epoch: 178 | LOSS: Train: 2.4745 Val: 5.3305 Test: 5.7955  | ROC: Train: 0.960, Val: 0.794, Test: 0.763\n",
            "Epoch: 179 | LOSS: Train: 2.4951 Val: 4.7317 Test: 5.1231  | ROC: Train: 0.959, Val: 0.781, Test: 0.751\n",
            "Epoch: 180 | LOSS: Train: 2.5151 Val: 5.0875 Test: 5.5809  | ROC: Train: 0.957, Val: 0.772, Test: 0.750\n",
            "Epoch: 181 | LOSS: Train: 2.4225 Val: 4.9297 Test: 5.5286  | ROC: Train: 0.962, Val: 0.774, Test: 0.745\n",
            "Epoch: 182 | LOSS: Train: 2.3391 Val: 5.0855 Test: 5.7352  | ROC: Train: 0.965, Val: 0.782, Test: 0.743\n",
            "Epoch: 183 | LOSS: Train: 2.3097 Val: 5.3284 Test: 5.8669  | ROC: Train: 0.964, Val: 0.773, Test: 0.744\n",
            "Epoch: 184 | LOSS: Train: 2.3271 Val: 5.4927 Test: 5.8698  | ROC: Train: 0.966, Val: 0.778, Test: 0.746\n",
            "Epoch: 185 | LOSS: Train: 2.3543 Val: 5.1363 Test: 5.5729  | ROC: Train: 0.965, Val: 0.772, Test: 0.735\n",
            "Epoch: 186 | LOSS: Train: 2.3760 Val: 5.8088 Test: 6.3803  | ROC: Train: 0.965, Val: 0.774, Test: 0.750\n",
            "Epoch: 187 | LOSS: Train: 2.3812 Val: 5.5290 Test: 6.0107  | ROC: Train: 0.967, Val: 0.772, Test: 0.746\n",
            "Epoch: 188 | LOSS: Train: 2.4147 Val: 5.4713 Test: 6.0319  | ROC: Train: 0.964, Val: 0.778, Test: 0.756\n",
            "Epoch: 189 | LOSS: Train: 2.2757 Val: 5.1537 Test: 5.7069  | ROC: Train: 0.969, Val: 0.778, Test: 0.749\n",
            "Epoch: 190 | LOSS: Train: 2.3339 Val: 5.3237 Test: 5.7750  | ROC: Train: 0.965, Val: 0.784, Test: 0.756\n",
            "Epoch: 191 | LOSS: Train: 2.2915 Val: 5.7721 Test: 6.1736  | ROC: Train: 0.969, Val: 0.774, Test: 0.748\n",
            "Epoch: 192 | LOSS: Train: 2.2442 Val: 5.5307 Test: 6.1909  | ROC: Train: 0.971, Val: 0.784, Test: 0.750\n",
            "Epoch: 193 | LOSS: Train: 2.2509 Val: 5.1921 Test: 5.4688  | ROC: Train: 0.969, Val: 0.766, Test: 0.744\n",
            "Epoch: 194 | LOSS: Train: 2.3450 Val: 5.8503 Test: 6.7249  | ROC: Train: 0.969, Val: 0.770, Test: 0.755\n",
            "Epoch: 195 | LOSS: Train: 2.2981 Val: 5.9737 Test: 6.4047  | ROC: Train: 0.970, Val: 0.761, Test: 0.746\n",
            "Epoch: 196 | LOSS: Train: 2.2483 Val: 6.1477 Test: 6.5314  | ROC: Train: 0.971, Val: 0.764, Test: 0.742\n",
            "Epoch: 197 | LOSS: Train: 2.1938 Val: 6.2329 Test: 6.9485  | ROC: Train: 0.973, Val: 0.779, Test: 0.756\n",
            "Epoch: 198 | LOSS: Train: 2.2821 Val: 5.0975 Test: 5.4123  | ROC: Train: 0.968, Val: 0.779, Test: 0.748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = f\"/content/drive/MyDrive/AI/covid-1/covid-GraphSage-4lay--200-d02--1.pt\"\n",
        "torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': train_loss,\n",
        "        }, PATH)\n",
        "torch.save(model,  PATH.strip(\".pt\") + \"-model1.pt\" )"
      ],
      "metadata": {
        "id": "hhWYMEdUVBy7"
      },
      "id": "hhWYMEdUVBy7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_GNNs_covid",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}